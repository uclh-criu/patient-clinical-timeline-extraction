{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import external libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path\n",
    "# Assuming the notebook is in a subdirectory of the project root (e.g., notebooks/)\n",
    "try:\n",
    "    # Get the absolute path of the current notebook (if running in an environment that supports it)\n",
    "    notebook_path = os.path.abspath(__file__) # Fails in some interactive environments\n",
    "except NameError:\n",
    "    # Fallback for interactive environments like Jupyter\n",
    "    notebook_path = os.path.abspath('.')\n",
    "\n",
    "project_root = os.path.dirname(notebook_path) # If notebook is in root\n",
    "if os.path.basename(project_root) == 'notebooks': # Check if we are in the 'notebooks' subdir\n",
    "    project_root = os.path.dirname(project_root) # Go one level up to the actual project root\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Project root added to path: {project_root}\")\n",
    "else:\n",
    "    print(f\"Project root already in path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from our modules\n",
    "from data.ClinicalNoteDataset import ClinicalNoteDataset\n",
    "from model_training.DiagnosisDateRelationModel import DiagnosisDateRelationModel\n",
    "from data.synthetic_data_generator import generate_dataset #this is used in train.py\n",
    "from utils.extraction_utils import extract_entities\n",
    "from utils.training_utils import train_model, evaluate_model, plot_training_curves, load_and_prepare_data, preprocess_note_for_prediction, create_prediction_dataset, predict_relationships, relate_diagnosis_to_date_rule_based\n",
    "from data.sample_note import CLINICAL_NOTE\n",
    "from config import *\n",
    "from model_training.training_config import *\n",
    "from model_training.Vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Generate or Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change dataset path to the one in the project root\n",
    "DATASET_PATH = os.path.join(project_root, 'data/synthetic_data.json')\n",
    "VOCAB_PATH = os.path.join(project_root, 'model_training/vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate or load dataset\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"Loading existing dataset from {DATASET_PATH}\")\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "print(f\"Dataset contains {len(dataset)} clinical notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate new dataset\n",
    "dataset = generate_dataset(num_notes=NUM_SAMPLES) #generate_dataset is from data/synthetic_clinical_notes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at it\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Prepare data for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare data for model training\n",
    "# within load_and_prepare_data we do the diagnoses extraction and date extraction\n",
    "features, labels, vocab_instance = load_and_prepare_data(DATASET_PATH, MAX_DISTANCE, Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if vocab was built successfully\n",
    "if vocab_instance:\n",
    "    print(f\"Successfully built vocabulary with {vocab_instance.n_words} words.\")\n",
    "else:\n",
    "    print(\"Error: Vocabulary building failed.\")\n",
    "    # Handle error, maybe exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(features)} examples with vocabulary size {vocab_instance.n_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary for later use in prediction\n",
    "#torch.save(vocab_instance, VOCAB_PATH)\n",
    "#print(f\"Saved vocabulary to {VOCAB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance of labels\n",
    "if len(labels) > 0:\n",
    "    positive = sum(labels)\n",
    "    negative = len(labels) - positive\n",
    "    print(f\"Class distribution: {positive} positive examples ({positive/len(labels)*100:.1f}%), {negative} negative examples ({negative/len(labels)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Warning: No examples found in the dataset!\")\n",
    "    exit(1)  # Exit with error code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Create Train / Val / Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train-validation-test split\n",
    "#Train and test split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and val split\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_features, train_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train: {len(train_features)}, Validation: {len(val_features)}, Test: {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ClinicalNoteDataset(train_features, train_labels, vocab_instance, MAX_CONTEXT_LEN, MAX_DISTANCE)\n",
    "val_dataset = ClinicalNoteDataset(val_features, val_labels, vocab_instance, MAX_CONTEXT_LEN, MAX_DISTANCE)\n",
    "test_dataset = ClinicalNoteDataset(test_features, test_labels, vocab_instance, MAX_CONTEXT_LEN, MAX_DISTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.features, train_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Intialize and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiagnosisDateRelationModel(\n",
    "        vocab_size=vocab_instance.n_words,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_instance.n_words, EMBEDDING_DIM, HIDDEN_DIM, LEARNING_RATE, NUM_EPOCHS, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_losses, val_losses, val_accs = train_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, NUM_EPOCHS, DEVICE, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curves(train_losses, val_losses, val_accs, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate model\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"Loading model from {MODEL_PATH}\")\n",
    "    model = DiagnosisDateRelationModel(\n",
    "        vocab_size=vocab_instance.n_words,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "else:\n",
    "    print(f\"Error: Model file {MODEL_PATH} not found!\")\n",
    "    print(\"Please run train_model.py first to train the model.\")\n",
    "\n",
    "# Load vocabulary\n",
    "if os.path.exists(VOCAB_PATH):\n",
    "    print(f\"Loading vocabulary from {VOCAB_PATH}\")\n",
    "    vocab = torch.load(VOCAB_PATH, weights_only=False)\n",
    "else:\n",
    "    print(f\"Error: Vocabulary file {VOCAB_PATH} not found!\")\n",
    "    print(\"Please run train_model.py first to generate the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to clinical note\n",
    "features = preprocess_note_for_prediction(CLINICAL_NOTE, MAX_DISTANCE)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_prediction_dataset(features, vocab_instance, DEVICE, MAX_DISTANCE, MAX_CONTEXT_LEN)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_relationships = predict_relationships(model, test_data)\n",
    "ml_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize by date\n",
    "date_dict = {}\n",
    "for rel in ml_relationships:\n",
    "    date = rel['date']\n",
    "    if date not in date_dict:\n",
    "        date_dict[date] = []\n",
    "    date_dict[date].append((rel['diagnosis'], rel['confidence']))\n",
    "\n",
    "date_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ML results\n",
    "print(\"\\nPatient Timeline from ML Model:\")\n",
    "for date, diagnoses in sorted(date_dict.items()):\n",
    "    print(f\"\\n{date}:\")\n",
    "    for diagnosis, confidence in sorted(diagnoses, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - {diagnosis} (confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with rule-based approach\n",
    "diagnoses, dates = extract_entities(CLINICAL_NOTE)\n",
    "diagnoses, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_based_relationships = relate_diagnosis_to_date_rule_based(diagnoses, dates)\n",
    "rule_based_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize by date\n",
    "rule_date_dict = {}\n",
    "for rel in rule_based_relationships:\n",
    "    date = rel['date']\n",
    "    if date not in rule_date_dict:\n",
    "        rule_date_dict[date] = []\n",
    "    rule_date_dict[date].append((rel['diagnosis'], rel['distance']))\n",
    "\n",
    "rule_date_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rule-based results\n",
    "print(\"\\nPatient Timeline from Rule-based Approach:\")\n",
    "for date, diagnoses in sorted(rule_date_dict.items()):\n",
    "    print(f\"\\n{date}:\")\n",
    "    for diagnosis, distance in sorted(diagnoses, key=lambda x: x[1]):\n",
    "        print(f\"  - {diagnosis} (distance: {distance} chars)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
