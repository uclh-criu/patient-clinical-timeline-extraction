{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42bf8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef260553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seeds\n",
    "set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f61b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "data = pd.read_csv(\"data/synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8574e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data formatting function\n",
    "\n",
    "def process_row(row):\n",
    "    text = row[\"note\"]\n",
    "\n",
    "    disorders = ast.literal_eval(row[\"extracted_disorders\"])\n",
    "    dates = ast.literal_eval(row[\"formatted_dates\"])\n",
    "    gold = ast.literal_eval(row[\"relationship_gold\"])\n",
    "\n",
    "    # Build lookup for gold relations: (disorder_pos, date_pos) -> relation_type\n",
    "    gold_map = {}\n",
    "    for g in gold:\n",
    "        date_pos = g[\"date_position\"]\n",
    "        for diag in g.get(\"diagnoses\", []):\n",
    "            gold_map[(diag[\"position\"], date_pos)] = \"diagnosis_date\"  # <-- adjust relation type if multiple types\n",
    "\n",
    "    samples = []\n",
    "    for d in disorders:\n",
    "        d_start, d_end = d[\"start\"], d[\"end\"]\n",
    "        disorder_text = text[d_start:d_end]\n",
    "\n",
    "        for dt in dates:\n",
    "            dt_start = dt.get(\"start\", None)\n",
    "            if dt_start is None:\n",
    "                dt_start = text.find(dt[\"original\"])\n",
    "            dt_end = dt_start + len(dt[\"original\"])\n",
    "            date_text = text[dt_start:dt_end]\n",
    "\n",
    "            # Label: check if pair is in gold_map\n",
    "            key = (d_start, dt_start)\n",
    "            label = gold_map.get(key, \"no_relation\")\n",
    "\n",
    "            # Insert entity markers (insert later span first)\n",
    "            marked = text\n",
    "            for span, token1, token2, ent_text, span_end in sorted(\n",
    "                [(d_start, \"[E1]\", \"[/E1]\", disorder_text, d_end),\n",
    "                 (dt_start, \"[E2]\", \"[/E2]\", date_text, dt_end)],\n",
    "                reverse=True\n",
    "            ):\n",
    "                marked = marked[:span] + f\"{token1} {ent_text} {token2}\" + marked[span_end:]\n",
    "\n",
    "            samples.append({\n",
    "                \"text\": text,\n",
    "                \"marked_text\": marked,\n",
    "                \"ent1_start\": d_start, \"ent1_end\": d_end,\n",
    "                \"ent2_start\": dt_start, \"ent2_end\": dt_end,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c6f9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>marked_text</th>\n",
       "      <th>ent1_start</th>\n",
       "      <th>ent1_end</th>\n",
       "      <th>ent2_start</th>\n",
       "      <th>ent2_end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "      <td>311</td>\n",
       "      <td>326</td>\n",
       "      <td>diagnosis_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "      <td>587</td>\n",
       "      <td>602</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>410</td>\n",
       "      <td>427</td>\n",
       "      <td>311</td>\n",
       "      <td>326</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>410</td>\n",
       "      <td>427</td>\n",
       "      <td>587</td>\n",
       "      <td>602</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>491</td>\n",
       "      <td>511</td>\n",
       "      <td>311</td>\n",
       "      <td>326</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "1  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "2  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "3  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "4  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "\n",
       "                                         marked_text  ent1_start  ent1_end  \\\n",
       "0  Ultrasound (30nd Jun 2024): no significant fin...          57        63   \n",
       "1  Ultrasound (30nd Jun 2024): no significant fin...          57        63   \n",
       "2  Ultrasound (30nd Jun 2024): no significant fin...         410       427   \n",
       "3  Ultrasound (30nd Jun 2024): no significant fin...         410       427   \n",
       "4  Ultrasound (30nd Jun 2024): no significant fin...         491       511   \n",
       "\n",
       "   ent2_start  ent2_end           label  \n",
       "0         311       326  diagnosis_date  \n",
       "1         587       602     no_relation  \n",
       "2         311       326     no_relation  \n",
       "3         587       602     no_relation  \n",
       "4         311       326     no_relation  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explode dataset into pairs\n",
    "all_samples = []\n",
    "for _, row in data.iterrows():\n",
    "    all_samples.extend(process_row(row))\n",
    "\n",
    "processed_df = pd.DataFrame(all_samples)\n",
    "processed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57bb4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define labels\n",
    "label_list = [\"no_relation\", \"diagnosis_date\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afed2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>marked_text</th>\n",
       "      <th>ent1_start</th>\n",
       "      <th>ent1_end</th>\n",
       "      <th>ent2_start</th>\n",
       "      <th>ent2_end</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "      <td>311</td>\n",
       "      <td>326</td>\n",
       "      <td>diagnosis_date</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "      <td>587</td>\n",
       "      <td>602</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>410</td>\n",
       "      <td>427</td>\n",
       "      <td>311</td>\n",
       "      <td>326</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>410</td>\n",
       "      <td>427</td>\n",
       "      <td>587</td>\n",
       "      <td>602</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>491</td>\n",
       "      <td>511</td>\n",
       "      <td>311</td>\n",
       "      <td>326</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "1  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "2  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "3  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "4  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "\n",
       "                                         marked_text  ent1_start  ent1_end  \\\n",
       "0  Ultrasound (30nd Jun 2024): no significant fin...          57        63   \n",
       "1  Ultrasound (30nd Jun 2024): no significant fin...          57        63   \n",
       "2  Ultrasound (30nd Jun 2024): no significant fin...         410       427   \n",
       "3  Ultrasound (30nd Jun 2024): no significant fin...         410       427   \n",
       "4  Ultrasound (30nd Jun 2024): no significant fin...         491       511   \n",
       "\n",
       "   ent2_start  ent2_end           label  label_id  \n",
       "0         311       326  diagnosis_date         1  \n",
       "1         587       602     no_relation         0  \n",
       "2         311       326     no_relation         0  \n",
       "3         587       602     no_relation         0  \n",
       "4         311       326     no_relation         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply labels\n",
    "processed_df[\"label_id\"] = processed_df[\"label\"].map(label2id)\n",
    "processed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0ef1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['marked_text', 'label_id'],\n",
       "    num_rows: 1242\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dataset\n",
    "dataset = Dataset.from_pandas(processed_df[[\"marked_text\", \"label_id\"]])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be13d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/test split\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b19070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizer with special tokens (+ model name)\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "special_tokens = {\"additional_special_tokens\": [\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d7af66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e8c6afb2e0444b9b863de96feadfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e28dd245fc481f9080e9cdc645be5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenization\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"marked_text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "tokenized = tokenized.rename_column(\"label_id\", \"labels\")\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1666dc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 214, 1: 35}\n"
     ]
    }
   ],
   "source": [
    "#Look at class distribution\n",
    "unique, counts = np.unique(tokenized[\"test\"][\"labels\"], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b49ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid2label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2id\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Resize embeddings so new tokens are usable\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n",
      "File \u001b[1;32mc:\\Users\\Satyam\\Downloads\\pituitary_adenoma\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Satyam\\Downloads\\pituitary_adenoma\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:316\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\Satyam\\Downloads\\pituitary_adenoma\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:5061\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   5051\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5052\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   5054\u001b[0m     (\n\u001b[0;32m   5055\u001b[0m         model,\n\u001b[0;32m   5056\u001b[0m         missing_keys,\n\u001b[0;32m   5057\u001b[0m         unexpected_keys,\n\u001b[0;32m   5058\u001b[0m         mismatched_keys,\n\u001b[0;32m   5059\u001b[0m         offload_index,\n\u001b[0;32m   5060\u001b[0m         error_msgs,\n\u001b[1;32m-> 5061\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5067\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5070\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5078\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   5079\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\Satyam\\Downloads\\pituitary_adenoma\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:5327\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   5324\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   5325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5326\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 5327\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   5328\u001b[0m     )\n\u001b[0;32m   5330\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[0;32m   5331\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[1;32mc:\\Users\\Satyam\\Downloads\\pituitary_adenoma\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:561\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m--> 561\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Satyam\\Downloads\\pituitary_adenoma\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1622\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m():\n\u001b[0;32m   1621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1622\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1623\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1624\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1625\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1626\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1627\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Resize embeddings so new tokens are usable\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9846473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run training\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set and print F1\n",
    "metrics = trainer.evaluate(tokenized[\"test\"])\n",
    "print(\"Test F1 (macro):\", metrics.get(\"f1\", metrics.get(\"f1_macro\")))\n",
    "print(\"All metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92153e4c",
   "metadata": {},
   "source": [
    "Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized[\"train\"]\n",
    "eval_dataset = tokenized[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for computing class weights for weighted CE loss (to handle imbalance)\n",
    "def compute_class_weights(ds: Dataset, n_labels: int):\n",
    "    if len(ds) == 0:\n",
    "        return torch.ones(n_labels)  # neutral\n",
    "    counts = Counter([int(x) for x in ds[\"labels\"]])\n",
    "    total = sum(counts.values())\n",
    "    weights = []\n",
    "    for i in range(n_labels):\n",
    "        # Inverse frequency (scaled): total / (n_labels * count_i)\n",
    "        # Clamp to avoid inf if a class is missing in train set.\n",
    "        c = max(1, counts.get(i, 0))\n",
    "        w = total / (n_labels * c)\n",
    "        weights.append(w)\n",
    "    # Normalize so mean weight ~= 1\n",
    "    mean_w = sum(weights) / len(weights)\n",
    "    weights = [w / mean_w for w in weights]\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08097997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute class weights\n",
    "class_weights = compute_class_weights(train_dataset, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60642f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom model: Bio_ClinicalBERT + span pooling between markers\n",
    "class BertRC(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward expected inputs (from Trainer):\n",
    "        input_ids:      [B, L]   torch.long\n",
    "        attention_mask: [B, L]   torch.long\n",
    "        labels:         [B]      torch.long\n",
    "\n",
    "    Internals:\n",
    "        last_hidden_state: [B, L, H]\n",
    "        span pooling: mask tokens strictly between [E1]...[/E1] and [E2]...[/E2]\n",
    "                       -> e1_emb, e2_emb: [B, H]\n",
    "        concat: [B, 2H] -> classifier -> logits: [B, num_labels]\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, tokenizer, num_labels: int, class_weights: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.backbone.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.hidden_size = self.backbone.config.hidden_size  # e.g., 768\n",
    "        self.dropout = nn.Dropout(self.backbone.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(2 * self.hidden_size, num_labels)\n",
    "\n",
    "        # Cache token IDs for markers\n",
    "        self.e1_open_id = tokenizer.convert_tokens_to_ids(\"[E1]\")\n",
    "        self.e1_close_id = tokenizer.convert_tokens_to_ids(\"[/E1]\")\n",
    "        self.e2_open_id = tokenizer.convert_tokens_to_ids(\"[E2]\")\n",
    "        self.e2_close_id = tokenizer.convert_tokens_to_ids(\"[/E2]\")\n",
    "\n",
    "        # Class weights for imbalance\n",
    "        if class_weights is not None:\n",
    "            self.register_buffer(\"class_weights\", class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _first_index(mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        mask: [B, L] bool\n",
    "        returns: [B] first True index (0 if none)\n",
    "        \"\"\"\n",
    "        # Convert to float and argmax: if no True, argmax returns 0 (handled later)\n",
    "        return mask.float().argmax(dim=1)\n",
    "\n",
    "    def _span_mean(\n",
    "        self,\n",
    "        hidden: torch.Tensor,      # [B, L, H]\n",
    "        input_ids: torch.Tensor,   # [B, L]\n",
    "        open_id: int,\n",
    "        close_id: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mean-pool tokens strictly between open and close markers.\n",
    "        Fallback: if span is empty or markers missing (e.g., truncation), use the open-marker embedding.\n",
    "\n",
    "        returns: [B, H]\n",
    "        \"\"\"\n",
    "        B, L, H = hidden.shape\n",
    "        pos = torch.arange(L, device=hidden.device).unsqueeze(0).expand(B, L)  # [B, L]\n",
    "\n",
    "        open_mask = (input_ids == open_id)    # [B, L]\n",
    "        close_mask = (input_ids == close_id)  # [B, L]\n",
    "\n",
    "        open_idx = self._first_index(open_mask)   # [B]\n",
    "        close_idx = self._first_index(close_mask) # [B]\n",
    "\n",
    "        # span_mask[b, t] = True iff open_idx[b] < t < close_idx[b]\n",
    "        span_mask = (pos > open_idx.unsqueeze(1)) & (pos < close_idx.unsqueeze(1))  # [B, L]\n",
    "\n",
    "        # Pool\n",
    "        denom = span_mask.sum(dim=1, keepdim=True).clamp_min(1)  # [B, 1]\n",
    "        span_sum = (hidden * span_mask.unsqueeze(-1)).sum(dim=1)  # [B, H]\n",
    "        span_mean = span_sum / denom  # [B, H]\n",
    "\n",
    "        # Fallback to open marker embedding if span empty or markers missing\n",
    "        has_tokens = span_mask.any(dim=1, keepdim=True)  # [B, 1] bool\n",
    "        open_emb = (hidden * open_mask.unsqueeze(-1)).sum(dim=1)  # [B, H]\n",
    "        e_emb = torch.where(has_tokens, span_mean, open_emb)      # [B, H]\n",
    "        return e_emb\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden = outputs.last_hidden_state  # [B, L, H]\n",
    "\n",
    "        # Pool entity and date spans\n",
    "        e1_emb = self._span_mean(last_hidden, input_ids, self.e1_open_id, self.e1_close_id)  # [B, H]\n",
    "        e2_emb = self._span_mean(last_hidden, input_ids, self.e2_open_id, self.e2_close_id)  # [B, H]\n",
    "\n",
    "        # Concatenate -> classify\n",
    "        x = torch.cat([e1_emb, e2_emb], dim=-1)  # [B, 2H]\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)  # [B, num_labels]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if hasattr(self, \"class_weights\") and self.class_weights is not None:\n",
    "                loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            else:\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics: accuracy + F1s\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0867b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = BertRC(model_name, tokenizer, num_labels=num_labels, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./rc_results\",\n",
    "    evaluation_strategy=\"epoch\" if len(eval_dataset) > 0 else \"no\",\n",
    "    save_strategy=\"epoch\" if len(eval_dataset) > 0 else \"no\",\n",
    "    load_best_model_at_end=True if len(eval_dataset) > 0 else False,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],  # turn off W&B/MLflow by default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset if len(train_dataset) > 0 else None,\n",
    "    eval_dataset=eval_dataset if len(eval_dataset) > 0 else None,\n",
    "    compute_metrics=compute_metrics if len(eval_dataset) > 0 else None,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] if len(eval_dataset) > 0 else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "if len(train_dataset) > 0:\n",
    "    trainer.train()\n",
    "    if len(eval_dataset) > 0:\n",
    "        metrics = trainer.evaluate()\n",
    "        print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
