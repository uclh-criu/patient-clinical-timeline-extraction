{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb4652a",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from general_utils import load_data, prepare_all_samples\n",
    "from bert_training_utils import (\n",
    "    build_gold_lookup, \n",
    "    get_label_for_pair, \n",
    "    create_training_pairs,\n",
    "    compute_class_weights, \n",
    "    downsample_classes, \n",
    "    upsample_classes, \n",
    "    handle_class_imbalance, \n",
    "    add_special_tokens, \n",
    "    tokenize_function\n",
    ")\n",
    "from bert_extractor_utils import preprocess_input, mark_entities_full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e56bce",
   "metadata": {},
   "source": [
    "Test BERT Pre-Processing & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad011617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Test Data\n",
    "def load_test_data():\n",
    "    \"\"\"Test 1: Load and prepare test data\"\"\"\n",
    "    df = load_data(\"../data/training_dataset.csv\")\n",
    "    samples = prepare_all_samples(df)\n",
    "    \n",
    "    print(\"=== Test 1: Data Loading ===\")\n",
    "    print(f\"Loaded {len(df)} documents\")\n",
    "    print(f\"Prepared {len(samples)} samples\")\n",
    "    \n",
    "    # Basic validation\n",
    "    sample = samples[0]\n",
    "    print(\"\\nFirst sample contents:\")\n",
    "    print(f\"- Number of entities: {len(sample['entities_list'])}\")\n",
    "    print(f\"- Number of absolute dates: {len(sample['dates'])}\")\n",
    "    print(f\"- Number of relative dates: {len(sample['relative_dates'])}\")\n",
    "    print(f\"- Number of relations: {len(sample['relations_json'])}\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Run test\n",
    "samples = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test Entity and Date Separation (Updated)\n",
    "def test_entity_date_separation(samples):\n",
    "    \"\"\"Test 2: Verify entities and dates are properly separated\"\"\"\n",
    "    print(\"=== Test 2: Entity-Date Separation ===\")\n",
    "    sample = samples[0]\n",
    "    \n",
    "    # More thorough date pattern check\n",
    "    date_patterns = [\n",
    "        r'\\d{1,2}/\\d{1,2}/\\d{2,4}',  # dd/mm/yyyy or mm/dd/yyyy\n",
    "        r'\\d{1,2}-\\d{1,2}-\\d{2,4}',   # dd-mm-yyyy or mm-dd-yyyy\n",
    "        r'\\d{1,2}/\\d{1,2}',           # dd/mm or mm/dd\n",
    "        r'\\d{4}',                      # yyyy\n",
    "    ]\n",
    "    \n",
    "    suspicious_entities = []\n",
    "    for e in sample['entities_list']:\n",
    "        for pattern in date_patterns:\n",
    "            if re.search(pattern, e['value']):\n",
    "                suspicious_entities.append(e)\n",
    "                break\n",
    "    \n",
    "    print(\"\\nChecking entities list:\")\n",
    "    print(f\"Total entities: {len(sample['entities_list'])}\")\n",
    "    if suspicious_entities:\n",
    "        print(\"WARNING: Found potential dates in entities list:\")\n",
    "        for e in suspicious_entities:\n",
    "            print(f\"- {e['value']} (CUI: {e.get('cui', 'N/A')})\")\n",
    "            print(f\"  Context: {sample['note_text'][max(0, e['start']-30):e['end']+30]}\")\n",
    "    else:\n",
    "        print(\"âœ“ No date patterns found in entities list\")\n",
    "    \n",
    "    # Verify dates are properly separated\n",
    "    print(\"\\nChecking date separation:\")\n",
    "    print(\"Absolute dates (first 3):\")\n",
    "    for d in sample['dates'][:3]:\n",
    "        print(f\"- {d['value']} (Position: {d['start']}-{d['end']})\")\n",
    "        print(f\"  Context: {sample['note_text'][max(0, d['start']-30):d['end']+30]}\")\n",
    "    \n",
    "    print(\"\\nRelative dates (all):\")\n",
    "    for rd in sample['relative_dates']:\n",
    "        print(f\"- {rd['value']} (Position: {rd['start']}-{rd['end']})\")\n",
    "        print(f\"  Context: {sample['note_text'][max(0, rd['start']-30):rd['end']+30]}\")\n",
    "\n",
    "# Run test\n",
    "test_entity_date_separation(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843de4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test Relations\n",
    "def test_relations(samples):\n",
    "    \"\"\"Test 3: Verify relation extraction and gold set creation\"\"\"\n",
    "    print(\"=== Test 3: Relations Testing ===\")\n",
    "    sample = samples[0]\n",
    "    \n",
    "    # Test gold set creation\n",
    "    gold_set = build_gold_lookup(sample['relations_json'])\n",
    "    print(f\"\\nGold set size: {len(gold_set)}\")\n",
    "    \n",
    "    # Test both absolute and relative date relations\n",
    "    print(\"\\nTesting absolute date relations:\")\n",
    "    abs_relations = [(r['entity'], r['date']) for r in sample['relations_json'] \n",
    "                    if r['date'] in [d['value'] for d in sample['dates']]]\n",
    "    print(f\"Number of absolute date relations: {len(abs_relations)}\")\n",
    "    print(\"First 3 absolute date relations:\")\n",
    "    for entity, date in abs_relations[:3]:\n",
    "        print(f\"- {entity} -> {date}\")\n",
    "    \n",
    "    print(\"\\nTesting relative date relations:\")\n",
    "    rel_relations = [(r['entity'], r['date']) for r in sample['relations_json'] \n",
    "                    if r['date'] in [rd['value'] for rd in sample['relative_dates']]]\n",
    "    print(f\"Number of relative date relations: {len(rel_relations)}\")\n",
    "    print(\"All relative date relations:\")\n",
    "    for entity, date in rel_relations:\n",
    "        print(f\"- {entity} -> {date}\")\n",
    "\n",
    "# Run test\n",
    "test_relations(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37952010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test Text Marking\n",
    "def test_text_marking(samples):\n",
    "    \"\"\"Test 4: Verify text marking for both absolute and relative dates\"\"\"\n",
    "    print(\"=== Test 4: Text Marking ===\")\n",
    "    sample = samples[0]\n",
    "    \n",
    "    # Get all relations\n",
    "    abs_relations = [(r['entity'], r['date']) for r in sample['relations_json'] \n",
    "                    if r['date'] in [d['value'] for d in sample['dates']]]\n",
    "    rel_relations = [(r['entity'], r['date']) for r in sample['relations_json'] \n",
    "                    if r['date'] in [rd['value'] for rd in sample['relative_dates']]]\n",
    "    \n",
    "    print(f\"\\nFound {len(abs_relations)} absolute date relations\")\n",
    "    print(f\"Found {len(rel_relations)} relative date relations\")\n",
    "    \n",
    "    # Test absolute date marking (first 3 examples)\n",
    "    print(\"\\nTesting absolute date marking:\")\n",
    "    for entity_text, date_text in abs_relations[:3]:\n",
    "        entity = next(e for e in sample['entities_list'] if e['value'] == entity_text)\n",
    "        date = next(d for d in sample['dates'] if d['value'] == date_text)\n",
    "        \n",
    "        print(f\"\\nPair: {entity_text} -> {date_text}\")\n",
    "        print(f\"Entity: {entity['value']} (Position: {entity['start']}-{entity['end']})\")\n",
    "        print(f\"Date: {date['value']} (Position: {date['start']}-{date['end']})\")\n",
    "        \n",
    "        marked = mark_entities_full_text(\n",
    "            sample['note_text'],\n",
    "            entity['start'], entity['end'],\n",
    "            date['start'], date['end'],\n",
    "            entity['value'], date['value']\n",
    "        )\n",
    "        \n",
    "        # Show focused context\n",
    "        start_pos = min(entity['start'], date['start'])\n",
    "        end_pos = max(entity['end'], date['end'])\n",
    "        context_start = max(0, start_pos - 30)\n",
    "        context_end = min(len(marked), end_pos + 30)\n",
    "        print(f\"Marked text: ...{marked[context_start:context_end]}...\")\n",
    "    \n",
    "    # Test relative date marking (all examples)\n",
    "    print(\"\\nTesting relative date marking:\")\n",
    "    for entity_text, date_text in rel_relations:\n",
    "        entity = next(e for e in sample['entities_list'] if e['value'] == entity_text)\n",
    "        rel_date = next(rd for rd in sample['relative_dates'] if rd['value'] == date_text)\n",
    "        \n",
    "        print(f\"\\nPair: {entity_text} -> {date_text}\")\n",
    "        print(f\"Entity: {entity['value']} (Position: {entity['start']}-{entity['end']})\")\n",
    "        print(f\"Date: {rel_date['value']} (Position: {rel_date['start']}-{rel_date['end']})\")\n",
    "        \n",
    "        marked = mark_entities_full_text(\n",
    "            sample['note_text'],\n",
    "            entity['start'], entity['end'],\n",
    "            rel_date['start'], rel_date['end'],\n",
    "            entity['value'], rel_date['value']\n",
    "        )\n",
    "        \n",
    "        # Show focused context\n",
    "        start_pos = min(entity['start'], rel_date['start'])\n",
    "        end_pos = max(entity['end'], rel_date['end'])\n",
    "        context_start = max(0, start_pos - 30)\n",
    "        context_end = min(len(marked), end_pos + 30)\n",
    "        print(f\"Marked text: ...{marked[context_start:context_end]}...\")\n",
    "\n",
    "# Run test\n",
    "test_text_marking(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test Training Pair Creation (Updated)\n",
    "def test_training_pairs(samples):\n",
    "    \"\"\"Test 5: Verify training pair creation and labeling\"\"\"\n",
    "    print(\"=== Test 5: Training Pair Creation ===\")\n",
    "    \n",
    "    # Create pairs from first sample only\n",
    "    sample = samples[0]\n",
    "    df = create_training_pairs([sample])\n",
    "    \n",
    "    print(\"\\nOverall pair statistics:\")\n",
    "    print(f\"Total pairs created: {len(df)}\")\n",
    "    print(\"\\nOverall label distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Get the actual dates used in each pair\n",
    "    rel_dates = set(rd['value'] for rd in sample['relative_dates'])\n",
    "    abs_dates = set(d['value'] for d in sample['dates'])\n",
    "    \n",
    "    # Filter based on the date being used in the pair, not just text content\n",
    "    def get_date_from_text(text):\n",
    "        # Extract the text between [E2] and [/E2]\n",
    "        import re\n",
    "        match = re.search(r'\\[E2\\](.*?)\\[/E2\\]', text)\n",
    "        return match.group(1).strip() if match else None\n",
    "    \n",
    "    df['date_used'] = df['marked_text'].apply(get_date_from_text)\n",
    "    rel_pairs = df[df['date_used'].isin(rel_dates)]\n",
    "    abs_pairs = df[df['date_used'].isin(abs_dates)]\n",
    "    \n",
    "    print(\"\\nAbsolute date pairs:\")\n",
    "    print(f\"Total absolute date pairs: {len(abs_pairs)}\")\n",
    "    print(\"Label distribution:\")\n",
    "    print(abs_pairs['label'].value_counts())\n",
    "    \n",
    "    print(\"\\nRelative date pairs:\")\n",
    "    print(f\"Total relative date pairs: {len(rel_pairs)}\")\n",
    "    print(\"Label distribution:\")\n",
    "    print(rel_pairs['label'].value_counts())\n",
    "    \n",
    "    # Show examples of both types\n",
    "    print(\"\\nExample positive absolute date pair:\")\n",
    "    if len(abs_pairs[abs_pairs['label'] == 1]) > 0:\n",
    "        pos_abs = abs_pairs[abs_pairs['label'] == 1].iloc[0]\n",
    "        print(f\"Text: ...{pos_abs['marked_text'][max(0, pos_abs['ent1_start']-30):min(len(pos_abs['marked_text']), pos_abs['ent1_end']+30)]}...\")\n",
    "    \n",
    "    print(\"\\nExample positive relative date pair:\")\n",
    "    if len(rel_pairs[rel_pairs['label'] == 1]) > 0:\n",
    "        pos_rel = rel_pairs[rel_pairs['label'] == 1].iloc[0]\n",
    "        print(f\"Text: ...{pos_rel['marked_text'][max(0, pos_rel['ent1_start']-30):min(len(pos_rel['marked_text']), pos_rel['ent1_end']+30)]}...\")\n",
    "\n",
    "# Run test\n",
    "test_training_pairs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test Class Balancing\n",
    "def test_class_balancing(samples):\n",
    "    \"\"\"Test 6: Verify class balancing methods\"\"\"\n",
    "    print(\"=== Test 6: Class Balancing ===\")\n",
    "    \n",
    "    # Create full training set\n",
    "    df = create_training_pairs(samples)\n",
    "    print(\"\\nOriginal class distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Test weighted balancing\n",
    "    weighted_df, weights = handle_class_imbalance(df, method='weighted')\n",
    "    print(\"\\nWeighted balancing:\")\n",
    "    print(f\"Class weights: {weights}\")\n",
    "    \n",
    "    # Test downsampling\n",
    "    down_df, _ = handle_class_imbalance(df, method='downsample')\n",
    "    print(\"\\nDownsampling results:\")\n",
    "    print(down_df['label'].value_counts())\n",
    "    \n",
    "    # Test upsampling\n",
    "    up_df, _ = handle_class_imbalance(df, method='upsample')\n",
    "    print(\"\\nUpsampling results:\")\n",
    "    print(up_df['label'].value_counts())\n",
    "\n",
    "# Run test\n",
    "test_class_balancing(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccac7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test Tokenization\n",
    "def test_tokenization():\n",
    "    \"\"\"Test 7: Verify tokenization and special token handling\"\"\"\n",
    "    print(\"=== Test 7: Tokenization ===\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "    print(\"\\nBefore adding special tokens:\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokenizer = add_special_tokens(tokenizer)\n",
    "    print(\"\\nAfter adding special tokens:\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
    "    \n",
    "    # Test tokenization with both absolute and relative dates\n",
    "    examples = [\n",
    "        {\"marked_text\": \"[E1]patient[/E1] seen on [E2]2023-01-01[/E2]\"},\n",
    "        {\"marked_text\": \"[E1]symptoms[/E1] started [E2]last week[/E2]\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTokenization tests:\")\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        encoded = tokenize_function(example, tokenizer, max_length=32)\n",
    "        decoded = tokenizer.decode(encoded['input_ids'])\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Original: {example['marked_text']}\")\n",
    "        print(f\"Decoded:  {decoded}\")\n",
    "        \n",
    "        # Verify markers are preserved\n",
    "        for marker in [\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\"]:\n",
    "            if marker not in decoded:\n",
    "                print(f\"WARNING: {marker} was lost in tokenization!\")\n",
    "\n",
    "# Run test\n",
    "test_tokenization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Test Token Length and Distance Analysis\n",
    "def test_token_lengths_and_distances():\n",
    "    \"\"\"Test 8: Analyze both document lengths and distances between entities/dates\"\"\"\n",
    "    print(\"=== Test 8: Token Length and Distance Analysis ===\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "    tokenizer = add_special_tokens(tokenizer)\n",
    "    \n",
    "    # Get first sample's pairs\n",
    "    sample = samples[0]\n",
    "    df = create_training_pairs([sample])\n",
    "    \n",
    "    # 1. Document Length Analysis\n",
    "    print(\"\\nDocument Length Analysis:\")\n",
    "    token_lengths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        tokens = tokenizer(row['marked_text'], truncation=False)['input_ids']\n",
    "        token_lengths.append(len(tokens))\n",
    "    \n",
    "    print(f\"Mean doc length: {sum(token_lengths)/len(token_lengths):.1f} tokens\")\n",
    "    print(f\"Max doc length: {max(token_lengths)} tokens\")\n",
    "    print(f\"Number of docs > 256 tokens: {sum(l > 256 for l in token_lengths)}\")\n",
    "    print(f\"Number of docs > 512 tokens: {sum(l > 512 for l in token_lengths)}\")\n",
    "\n",
    "    # 2. Distance Analysis for Positive Pairs\n",
    "    print(\"\\nDistance Analysis for True Relations:\")\n",
    "    \n",
    "    # Get positive pairs\n",
    "    positive_pairs = df[df['label'] == 1]\n",
    "    \n",
    "    # Calculate distances for positive pairs\n",
    "    distances = []\n",
    "    abs_distances = []\n",
    "    rel_distances = []\n",
    "    \n",
    "    for idx, row in positive_pairs.iterrows():\n",
    "        text = row['marked_text']\n",
    "        e1_pos = text.find('[E1]')\n",
    "        e2_pos = text.find('[E2]')\n",
    "        distance = abs(e2_pos - e1_pos)\n",
    "        distances.append(distance)\n",
    "        \n",
    "        # Better relative date detection\n",
    "        relative_patterns = ['last', 'ago', 'today', 'month', 'year', 'week', \n",
    "                           'previous', 'next', 'current']\n",
    "        # Extract the date text between [E2] and [/E2]\n",
    "        date_start = text.find('[E2]') + 4\n",
    "        date_end = text.find('[/E2]')\n",
    "        date_text = text[date_start:date_end] if date_start > 0 and date_end > 0 else \"\"\n",
    "        \n",
    "        if any(pattern in date_text.lower() for pattern in relative_patterns):\n",
    "            rel_distances.append(distance)\n",
    "        else:\n",
    "            abs_distances.append(distance)\n",
    "    \n",
    "    print(\"\\nAll Positive Pairs:\")\n",
    "    print(f\"Total positive pairs: {len(distances)}\")\n",
    "    if distances:\n",
    "        print(f\"Mean distance: {sum(distances)/len(distances):.1f} chars\")\n",
    "        print(f\"Min distance: {min(distances)} chars\")\n",
    "        print(f\"Max distance: {max(distances)} chars\")\n",
    "    \n",
    "    print(\"\\nAbsolute Date Pairs:\")\n",
    "    print(f\"Total pairs: {len(abs_distances)}\")\n",
    "    if abs_distances:\n",
    "        print(f\"Mean distance: {sum(abs_distances)/len(abs_distances):.1f} chars\")\n",
    "        print(f\"Min distance: {min(abs_distances)} chars\")\n",
    "        print(f\"Max distance: {max(abs_distances)} chars\")\n",
    "    \n",
    "    print(\"\\nRelative Date Pairs:\")\n",
    "    print(f\"Total pairs: {len(rel_distances)}\")\n",
    "    if rel_distances:\n",
    "        print(f\"Mean distance: {sum(rel_distances)/len(rel_distances):.1f} chars\")\n",
    "        print(f\"Min distance: {min(rel_distances)} chars\")\n",
    "        print(f\"Max distance: {max(rel_distances)} chars\")\n",
    "    \n",
    "    # Show examples of closest and furthest pairs\n",
    "    print(\"\\nExample Pairs:\")\n",
    "    closest_idx = distances.index(min(distances))\n",
    "    furthest_idx = distances.index(max(distances))\n",
    "    \n",
    "    closest_pair = positive_pairs.iloc[closest_idx]\n",
    "    furthest_pair = positive_pairs.iloc[furthest_idx]\n",
    "    \n",
    "    print(\"\\nClosest Pair:\")\n",
    "    print(f\"Distance: {min(distances)} chars\")\n",
    "    closest_text = closest_pair['marked_text']\n",
    "    e1_pos = closest_text.find('[E1]')\n",
    "    e2_pos = closest_text.find('[/E2]')\n",
    "    start = max(0, e1_pos - 30)\n",
    "    end = min(len(closest_text), e2_pos + 30)\n",
    "    print(f\"Text: ...{closest_text[start:end]}...\")\n",
    "    \n",
    "    print(\"\\nFurthest Pair:\")\n",
    "    print(f\"Distance: {max(distances)} chars\")\n",
    "    furthest_text = furthest_pair['marked_text']\n",
    "    e1_pos = furthest_text.find('[E1]')\n",
    "    e2_pos = furthest_text.find('[/E2]')\n",
    "    start = max(0, e1_pos - 30)\n",
    "    end = min(len(furthest_text), e2_pos + 30)\n",
    "    print(f\"Text: ...{furthest_text[start:end]}...\")\n",
    "\n",
    "# Run test\n",
    "test_token_lengths_and_distances()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
