{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb4652a",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from general_utils import load_data, prepare_all_samples\n",
    "from bert_training_utils import build_gold_lookup, get_label_for_pair, create_training_pairs, compute_class_weights, balance_classes, handle_class_imbalance, add_special_tokens, tokenize_function\n",
    "from bert_extractor_utils import preprocess_input, mark_entities_full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e56bce",
   "metadata": {},
   "source": [
    "Test BERT Pre-Processing & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad011617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 samples\n"
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "df = load_data(\"../data/training_dataset.csv\")  # adjust path if needed\n",
    "samples = prepare_all_samples(df)\n",
    "print(f\"Loaded {len(samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c3a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample info:\n",
      "Note text (first 100 chars): Ultrasound (30nd Jun 2024): no significant findings.imp: asthma\n",
      "\n",
      "She denies any nausea, vomiting, or...\n",
      "Selected entity: {'id': 308252, 'value': 'pituitary_adenoma', 'cui': '254956000', 'start': 410, 'end': 427}\n",
      "Selected date: {'id': 308321, 'value': '12nd Sep 2024', 'start': 363, 'end': 376}\n"
     ]
    }
   ],
   "source": [
    "# Get first sample and find a positive relation\n",
    "sample = samples[0]\n",
    "gold_map = build_gold_lookup(sample['relations_json'])\n",
    "\n",
    "# Find entity and date that form a relation\n",
    "for rel in sample['relations_json']:\n",
    "    entity = next((e for e in sample['entities_list'] if e['value'] == rel['entity']), None)\n",
    "    date = next((d for d in sample['dates'] if d['value'] == rel['date']), None)\n",
    "    if entity and date:\n",
    "        break\n",
    "\n",
    "print(\"\\nSample info:\")\n",
    "print(f\"Note text (first 100 chars): {sample['note_text'][:100]}...\")\n",
    "print(f\"Selected entity: {entity}\")\n",
    "print(f\"Selected date: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aeee6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing build_gold_lookup...\n",
      "Gold map: {('pituitary_adenoma', '12nd Sep 2024'), ('rheumatoid_arthritis', \"16 Sep'24\"), ('headache', '23rd Oct 2024'), ('GERD', '17.12.24')}\n"
     ]
    }
   ],
   "source": [
    "# Test build_gold_lookup\n",
    "print(\"Testing build_gold_lookup...\")\n",
    "print(f\"Gold map: {gold_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7deb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_label_for_pair...\n",
      "Label for pituitary_adenoma + 12nd Sep 2024: relation\n"
     ]
    }
   ],
   "source": [
    "# Test get_label_for_pair\n",
    "print(\"Testing get_label_for_pair...\")\n",
    "label = get_label_for_pair(entity['value'], date['value'], gold_map)  # Use values instead of starts\n",
    "print(f\"Label for {entity['value']} + {date['value']}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029eae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mark_entities_full_text...\n",
      "Marked text around entities:\n",
      "2nd Aug 2024): reveals asthma.imp: asthma\n",
      "\n",
      "X-ray ([E2] 12nd Sep 2024 [/E2]): shows 3.1cm mass in brain.imp: [E1] pituitary_adenoma [/E1]\n",
      "\n",
      "CLINIC VI\n"
     ]
    }
   ],
   "source": [
    "# Test mark_entities_full_text\n",
    "print(\"Testing mark_entities_full_text...\")\n",
    "marked = mark_entities_full_text(\n",
    "    sample['note_text'],\n",
    "    entity['start'], entity['end'],\n",
    "    date['start'], date['end'],\n",
    "    entity['value'], date['value']\n",
    ")\n",
    "print(f\"Marked text around entities:\")\n",
    "context_start = max(0, min(entity['start'], date['start']) - 50)\n",
    "context_end = min(len(marked), max(entity['start'], date['end']) + 50)\n",
    "print(marked[context_start:context_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6641ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocess_input...\n",
      "\n",
      "Preprocessed input around entities:\n",
      "2nd Aug 2024): reveals asthma.imp: asthma\n",
      "\n",
      "X-ray ([E2] 12nd Sep 2024 [/E2]): shows 3.1cm mass in brain.imp: [E1] pituitary_adenoma [/E1]\n",
      "\n",
      "CLINIC VI\n"
     ]
    }
   ],
   "source": [
    "# Test preprocessing\n",
    "print(\"Testing preprocess_input...\")\n",
    "preprocessed = preprocess_input(sample['note_text'], entity, date)\n",
    "print(\"\\nPreprocessed input around entities:\")\n",
    "context_start = max(0, min(entity['start'], date['start']) - 50)\n",
    "context_end = min(len(preprocessed['marked_text']), max(entity['start'], date['end']) + 50)\n",
    "print(preprocessed['marked_text'][context_start:context_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c0b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing create_training_pairs...\n",
      "\n",
      "Created 384 training pairs\n",
      "\n",
      "Sample columns: ['text', 'marked_text', 'ent1_start', 'ent1_end', 'ent2_start', 'ent2_end', 'label', 'patient_id', 'note_id', 'distance']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    379\n",
      "1      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First positive example:\n",
      "Text snippet: ... ([E2] 16 Sep'24 [/E2]): nausea/vomiting worsening confirmed [E1] rheumatoid_arthritis [/E1] switch to aspirin\n",
      "\n",
      "Past med...\n"
     ]
    }
   ],
   "source": [
    "# Test create_training_pairs\n",
    "print(\"Testing create_training_pairs...\")\n",
    "df = create_training_pairs([sample])\n",
    "print(f\"\\nCreated {len(df)} training pairs\")\n",
    "print(\"\\nSample columns:\", df.columns.tolist())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nFirst positive example:\")\n",
    "pos = df[df['label'] == 1].iloc[0]\n",
    "start = max(0, pos['ent1_start']-50)\n",
    "end = min(len(pos['marked_text']), pos['ent1_end']+50)\n",
    "print(f\"Text snippet: ...{pos['marked_text'][start:end]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05433186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing compute_class_weights...\n",
      "\n",
      "Class weights: tensor([0.0260, 1.9740])\n",
      "(Should be ~1 on average, higher for minority class)\n",
      "\n",
      "Class distribution:\n",
      "Class 0: count=379, weight=0.026\n",
      "Class 1: count=5, weight=1.974\n"
     ]
    }
   ],
   "source": [
    "# Test class weight computation\n",
    "print(\"Testing compute_class_weights...\")\n",
    "weights = compute_class_weights(df, num_labels=2)\n",
    "print(f\"\\nClass weights: {weights}\")\n",
    "print(\"(Should be ~1 on average, higher for minority class)\")\n",
    "\n",
    "# Verify weights are working as expected\n",
    "counts = df['label'].value_counts()\n",
    "print(\"\\nClass distribution:\")\n",
    "for label, count in counts.items():\n",
    "    weight = weights[int(label)]\n",
    "    print(f\"Class {label}: count={count}, weight={weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b03bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing balance_classes...\n",
      "\n",
      "Before balancing:\n",
      "label\n",
      "0    379\n",
      "1      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After balancing:\n",
      "label\n",
      "0    5\n",
      "1    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test downsampling\n",
    "print(\"Testing balance_classes...\")\n",
    "balanced_df = balance_classes(df, ratio=1.0)\n",
    "print(\"\\nBefore balancing:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nAfter balancing:\")\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567a1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing handle_class_imbalance...\n",
      "\n",
      "Method: weighted\n",
      "Returned weights: tensor([0.0260, 1.9740])\n",
      "Class distribution (unchanged):\n",
      "label\n",
      "0    379\n",
      "1      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Method: downsample\n",
      "Returned weights: None\n",
      "Class distribution (balanced):\n",
      "label\n",
      "0    5\n",
      "1    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test both imbalance handling methods\n",
    "print(\"Testing handle_class_imbalance...\")\n",
    "\n",
    "print(\"\\nMethod: weighted\")\n",
    "weighted_df, w_weights = handle_class_imbalance(df, method='weighted')\n",
    "print(f\"Returned weights: {w_weights}\")\n",
    "print(\"Class distribution (unchanged):\")\n",
    "print(weighted_df['label'].value_counts())\n",
    "\n",
    "print(\"\\nMethod: downsample\")\n",
    "down_df, d_weights = handle_class_imbalance(df, method='downsample')\n",
    "print(f\"Returned weights: {d_weights}\")\n",
    "print(\"Class distribution (balanced):\")\n",
    "print(down_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caecb3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tokenizer functions...\n",
      "\n",
      "Before adding special tokens:\n",
      "Vocab size: 28996\n",
      "Special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "\n",
      "After adding special tokens:\n",
      "Vocab size: 29000\n",
      "Special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[E1]', '[/E1]', '[E2]', '[/E2]']\n",
      "\n",
      "Tokenization roundtrip:\n",
      "Original: [E1] asthma [/E1] was diagnosed on [E2] 2024-01-01 [/E2]\n",
      "Decoded: [CLS] [E1] asthma [/E1] was diagnosed on [E2] 2024 - 01 - 01 [/E2] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Test add_special_tokens and tokenize_function\n",
    "print(\"Testing tokenizer functions...\")\n",
    "\n",
    "# Load base tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "print(\"\\nBefore adding special tokens:\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer = add_special_tokens(tokenizer)\n",
    "print(\"\\nAfter adding special tokens:\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n",
    "\n",
    "# Verify special tokens work\n",
    "example = {\"marked_text\": \"[E1] asthma [/E1] was diagnosed on [E2] 2024-01-01 [/E2]\"}\n",
    "encoded = tokenize_function(example, tokenizer, max_length=32)\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(\"\\nTokenization roundtrip:\")\n",
    "print(\"Original:\", example['marked_text'])\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "# Check if special tokens are preserved\n",
    "for token in [\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\"]:\n",
    "    if token not in decoded:\n",
    "        print(f\"WARNING: {token} was lost in tokenization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
