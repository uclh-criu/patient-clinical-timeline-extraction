{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add utils to path\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from date_extractor_utils import clean_value, extract_absolute_dates, normalise_relative, extract_relative_dates\n",
    "from general_utils import load_data\n",
    "from bert_relative_date_utils import predict_relative_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed2da3",
   "metadata": {},
   "source": [
    "Test Absolute Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on simple example\n",
    "test_text = \"Patient was seen on 15/06/2025 for follow-up. Next appointment scheduled for January 4th, 2026.\"\n",
    "\n",
    "absolute_dates = extract_absolute_dates(test_text)\n",
    "\n",
    "print(\"Results:\")\n",
    "for date in absolute_dates:\n",
    "    print(f\"  '{date['value']}' -> (start: {date['start']}, end: {date['end']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on longer example with various date formats\n",
    "test_text = \"\"\"\n",
    "Various date formats:\n",
    "1. Standard formats:\n",
    "   - 15/06/2025\n",
    "   - 2025-06-15\n",
    "   - 15-06-2025\n",
    "   \n",
    "2. Month name formats:\n",
    "   - June 15, 2025\n",
    "   - 15 June 2025\n",
    "   - Jun 15, 2025\n",
    "   \n",
    "3. Mixed in text:\n",
    "   The patient was seen on 15/06/2025 and had a follow-up on June 15, 2025.\n",
    "   Next appointment scheduled for January 4th, 2026.\n",
    "\"\"\"\n",
    "\n",
    "absolute_dates = extract_absolute_dates(test_text)\n",
    "print(\"\\nResults:\")\n",
    "for date in absolute_dates:\n",
    "    print(f\"  '{date['value']}' -> (start: {date['start']}, end: {date['end']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases\n",
    "edge_cases = [\n",
    "    \"\",  # Empty string\n",
    "    \"No dates here\",  # No dates\n",
    "    \"Invalid dates: 35/13/2025, 00/00/0000\",  # Invalid dates\n",
    "    \"Partial dates: June 2025, 2025\",  # Partial dates\n",
    "]\n",
    "\n",
    "for text in edge_cases:\n",
    "    dates = extract_absolute_dates(text)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Found {len(dates)} dates:\")\n",
    "    for date in dates:\n",
    "        print(f\"  '{date['value']}' -> (start: {date['start']}, end: {date['end']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on actual dataset sample\n",
    "df = pd.read_csv(\"../data/dataset_synthetic1.csv\")\n",
    "sample_text = df.iloc[0]['note_text']\n",
    "\n",
    "dates = extract_absolute_dates(sample_text)\n",
    "\n",
    "print(f\"Text: {sample_text}...\")\n",
    "print(f\"\\nFound {len(dates)} dates:\")\n",
    "for date in dates:\n",
    "    print(f\"  '{date['value']}' -> (start: {date['start']}, end: {date['end']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73251639",
   "metadata": {},
   "source": [
    "Test Relative Dates (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a93e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one example\n",
    "test_text = \"Patient was seen last week for follow-up. Next appointment scheduled for tomorrow. Symptoms started 3 days ago.\"\n",
    "relative_dates = extract_relative_dates(test_text)\n",
    "relative_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on comprehensive examples to verify all pattern types\n",
    "test_text = \"\"\"\n",
    "Patient was seen last week for follow-up. \n",
    "Next appointment scheduled for tomorrow. \n",
    "Symptoms started 3 days ago.\n",
    "Last visit was on Monday.\n",
    "Previous checkup was 2 weeks earlier.\n",
    "Past few days have been difficult.\n",
    "Several months ago the condition worsened.\n",
    "Earlier this week the patient improved.\n",
    "Last visit was productive.\n",
    "Next few days will be critical.\n",
    "\"\"\"\n",
    "\n",
    "results = extract_relative_dates(test_text)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for result in results:\n",
    "    print(f\"  '{result['value']}' -> (pattern: {result['pattern_type']})\")\n",
    "\n",
    "print(f\"\\nTotal patterns found: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8aa30",
   "metadata": {},
   "source": [
    "Test Against Labelled Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset using existing load_data function\n",
    "df = load_data(\"../data/training_dataset_nph.csv\")\n",
    "print(f\"Main dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94704c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned gold standard data we generated\n",
    "df_clean_gold = pd.read_csv(\"../data/relative_date_gold.csv\")\n",
    "\n",
    "# Create a dictionary of maps, one for each validation category\n",
    "category_maps = {}\n",
    "for category in df_clean_gold['is_valid'].unique():\n",
    "    category_maps[category] = (\n",
    "        df_clean_gold[df_clean_gold['is_valid'] == category]\n",
    "        .groupby('doc_id')['date_value']\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "# For convenience, keep the valid_rel_dates_map for the evaluation cell\n",
    "valid_rel_dates_map = category_maps.get('YES', {})\n",
    "\n",
    "print(f\"Loaded gold standard data for {len(df_clean_gold['is_valid'].unique())} categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37061d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set relative date method - can be bert or regex\n",
    "relative_date_method = 'regex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model load path\n",
    "if relative_date_method == 'bert':\n",
    "    model_load_path = '../models/bert_model_relative_dates/'\n",
    "else:\n",
    "    model_load_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned relative date extractor (if using BERT)\n",
    "if relative_date_method == 'bert':\n",
    "    tokenizer_rel = AutoTokenizer.from_pretrained(model_load_path)\n",
    "    model_rel = AutoModelForTokenClassification.from_pretrained(model_load_path)\n",
    "    model_rel.eval()\n",
    "    print(\"BERT relative date model loaded successfully!\")\n",
    "else:\n",
    "    print(\"Using regex-based relative date extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "abs_results = []\n",
    "rel_results = []\n",
    "\n",
    "# Loop through each note\n",
    "for _, row in df.iterrows():\n",
    "    text = row['note_text']\n",
    "    doc_id = row.get('doc_id') # Retrieve doc_id for map lookup\n",
    "\n",
    "    # Absolute dates\n",
    "    validated_abs = row.get('dates_json', []) or []\n",
    "    if isinstance(validated_abs, str):\n",
    "        try:\n",
    "            validated_abs = json.loads(validated_abs)\n",
    "        except json.JSONDecodeError:\n",
    "            validated_abs = []\n",
    "    \n",
    "    gold_abs_values = {clean_value(d['value']) for d in validated_abs if isinstance(d, dict) and d.get('value')}\n",
    "    \n",
    "    # Relative dates\n",
    "    validated_rel_list = valid_rel_dates_map.get(doc_id, [])\n",
    "    gold_rel_values = {normalise_relative(v) for v in validated_rel_list}\n",
    "    pred_abs = extract_absolute_dates(text)\n",
    "\n",
    "    if relative_date_method == 'bert':\n",
    "        pred_rel = predict_relative_dates(text, model_rel, tokenizer_rel)\n",
    "    elif relative_date_method == 'regex':\n",
    "        pred_rel = extract_relative_dates(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: {relative_date_method}. Must be either 'bert' or 'regex'.\")\n",
    "\n",
    "    # Normalise predicted values\n",
    "    pred_abs_values = {clean_value(p['value']) for p in pred_abs if isinstance(p, dict) and p.get('value')}\n",
    "    pred_rel_values = {normalise_relative(p['value']) for p in pred_rel if isinstance(p, dict) and p.get('value')}\n",
    "\n",
    "    # Compare sets using same normalisation\n",
    "    tp_abs = len(pred_abs_values & gold_abs_values)\n",
    "    fp_abs = len(pred_abs_values - gold_abs_values)\n",
    "    fn_abs = len(gold_abs_values - pred_abs_values)\n",
    "\n",
    "    tp_rel = len(pred_rel_values & gold_rel_values)\n",
    "    fp_rel = len(pred_rel_values - gold_rel_values)\n",
    "    fn_rel = len(gold_rel_values - pred_rel_values)\n",
    "\n",
    "    abs_results.append((tp_abs, fp_abs, fn_abs))\n",
    "    rel_results.append((tp_rel, fp_rel, fn_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c471ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def compute_metrics(results):\n",
    "    tp = sum(r[0] for r in results)\n",
    "    fp = sum(r[1] for r in results)\n",
    "    fn = sum(r[2] for r in results)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1, tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "abs_precision, abs_recall, abs_f1, tp_abs, fp_abs, fn_abs = compute_metrics(abs_results)\n",
    "rel_precision, rel_recall, rel_f1, tp_rel, fp_rel, fn_rel = compute_metrics(rel_results)\n",
    "\n",
    "print(\"=== Absolute Dates ===\")\n",
    "print(f\"TP={tp_abs}, FP={fp_abs}, FN={fn_abs}\")\n",
    "print(f\"Precision={abs_precision:.3f}, Recall={abs_recall:.3f}, F1={abs_f1:.3f}\")\n",
    "\n",
    "print(\"\\n=== Relative Dates ===\")\n",
    "print(f\"TP={tp_rel}, FP={fp_rel}, FN={fn_rel}\")\n",
    "print(f\"Precision={rel_precision:.3f}, Recall={rel_recall:.3f}, F1={rel_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6065e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug absolute dates (first 10 rows)\n",
    "for _, row in df.head(10).iterrows():\n",
    "    text = row['note_text']\n",
    "\n",
    "    abs_data = row.get('dates_json')\n",
    "    if isinstance(abs_data, str):\n",
    "        try:\n",
    "            abs_data = json.loads(abs_data)\n",
    "        except json.JSONDecodeError:\n",
    "            abs_data = []\n",
    "\n",
    "    # Extract validated and predicted values\n",
    "    gold_abs = {d['value'].strip() for d in abs_data if isinstance(d, dict) and 'value' in d}\n",
    "    pred_abs = {p['value'].strip() for p in extract_absolute_dates(text) if isinstance(p, dict) and 'value' in p}\n",
    "\n",
    "    # Only show rows where something exists\n",
    "    if gold_abs or pred_abs:\n",
    "        print(f\"\\nDoc {row.get('doc_id', 'N/A')}\")\n",
    "        print(\"Validated absolute:\", gold_abs)\n",
    "        print(\"Predicted absolute:\", pred_abs)\n",
    "        print(\"Overlap:\", gold_abs & pred_abs)\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug relative date extraction with full category breakdown\n",
    "for _, row in df.iterrows():\n",
    "    text = row['note_text']\n",
    "    doc_id = row.get('doc_id')\n",
    "\n",
    "    # Get Gold Standard Sets from Maps\n",
    "    gold_sets = {\n",
    "        category: {v.lower() for v in maps.get(doc_id, [])}\n",
    "        for category, maps in category_maps.items()\n",
    "    }\n",
    "    gold_yes = gold_sets.get('YES', set())\n",
    "    all_gold_values = set.union(*gold_sets.values()) # Combine all gold values for checking new FPs\n",
    "\n",
    "    # Prediction\n",
    "    pred_raw = {p['value'].lower() for p in extract_relative_dates(text)}\n",
    "\n",
    "    # Analysis\n",
    "    if not (gold_yes or pred_raw):\n",
    "        continue # Skip docs with no gold dates and no predictions\n",
    "\n",
    "    print(f\"\\n--- Doc {doc_id} ---\")\n",
    "    print(f\"Gold (YES): {gold_yes if gold_yes else 'None'}\")\n",
    "    print(f\"Predicted:  {pred_raw if pred_raw else 'None'}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # True Positives\n",
    "    tp = gold_yes & pred_raw\n",
    "    print(f\"Correctly Predicted (TP): {tp if tp else 'None'}\")\n",
    "\n",
    "    # False Negatives\n",
    "    fn = gold_yes - pred_raw\n",
    "    print(f\"Missed (FN): {fn if fn else 'None'}\")\n",
    "\n",
    "    # False Positives Breakdown\n",
    "    fp = pred_raw - gold_yes\n",
    "    if fp:\n",
    "        print(\"Incorrectly Predicted (FP):\")\n",
    "        for category, gold_values in gold_sets.items():\n",
    "            if category == 'YES': continue\n",
    "            fp_category = fp & gold_values\n",
    "            if fp_category:\n",
    "                print(f\"  - Is {category}: {fp_category}\")\n",
    "        \n",
    "        # Catch any FPs that were not in our gold standard at all\n",
    "        fp_novel = fp - all_gold_values\n",
    "        if fp_novel:\n",
    "            print(f\"  - Is Novel (Not in Gold): {fp_novel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of FPs and FNs\n",
    "\n",
    "# Set display_mode to 'FP', 'FN', or 'BOTH' to control the output\n",
    "display_mode = 'BOTH'\n",
    "\n",
    "# Store all errors for a final summary\n",
    "all_errors = {'FP': [], 'FN': []}\n",
    "\n",
    "# Loop through every row in the dataframe\n",
    "for _, row in df.iterrows():\n",
    "    text = row['note_text']\n",
    "    doc_id = row.get('doc_id')\n",
    "\n",
    "    # Get Gold Standard Sets from Maps\n",
    "    gold_raw_map = {\n",
    "        category: {v.lower() for v in maps.get(doc_id, [])}\n",
    "        for category, maps in category_maps.items()\n",
    "    }\n",
    "    gold_norm_yes = {normalise_relative(v) for v in gold_raw_map.get('YES', set())}\n",
    "    \n",
    "    # Prediction\n",
    "    pred_raw_set = {p['value'].lower() for p in extract_relative_dates(text)}\n",
    "    pred_norm_set = {normalise_relative(v) for v in pred_raw_set}\n",
    "\n",
    "    # Identify False Positives and False Negatives\n",
    "    false_positives = pred_norm_set - gold_norm_yes\n",
    "    false_negatives = gold_norm_yes - pred_norm_set\n",
    "\n",
    "    # Log and Print Errors\n",
    "    if display_mode in ['FP', 'BOTH'] and false_positives:\n",
    "        all_errors['FP'].extend(list(false_positives))\n",
    "        for val_norm in false_positives:\n",
    "            # Find the original raw value that corresponds to the normalized FP\n",
    "            raw_val = next((r for r in pred_raw_set if normalise_relative(r) == val_norm), val_norm)\n",
    "            print(f\"Doc {doc_id}: FP > Gold: None > Predicted: '{raw_val}' (Norm: '{val_norm}') > Evaluation: Missed\")\n",
    "            \n",
    "    if display_mode in ['FN', 'BOTH'] and false_negatives:\n",
    "        all_errors['FN'].extend(list(false_negatives))\n",
    "        for val_norm in false_negatives:\n",
    "            # Find the original raw gold value for display\n",
    "            raw_val = next((r for r in gold_raw_map.get('YES', set()) if normalise_relative(r) == val_norm), val_norm)\n",
    "            print(f\"Doc {doc_id}: FN > Gold: '{raw_val}' (Norm: '{val_norm}') > Predicted: None > Evaluation: Missed\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Total False Positives: {len(all_errors['FP'])}\")\n",
    "print(f\"Total False Negatives: {len(all_errors['FN'])}\")\n",
    "print(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
