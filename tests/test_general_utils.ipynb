{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dada8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add utils to path\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from general_utils import (\n",
    "    parse_jsonish,\n",
    "    load_data,\n",
    "    prepare_sample,\n",
    "    prepare_all_samples,\n",
    "    get_entity_date_pairs,\n",
    "    calculate_metrics\n",
    ")\n",
    "\n",
    "from bert_training_utils import create_training_pairs\n",
    "from naive_extractor_utils import naive_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df42518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset to test\n",
    "#test_df_path = \"../data/training_dataset.csv\"\n",
    "test_df_path = \"../data/training_dataset_synthetic2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d687ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test load_data\n",
    "def test_load_data():\n",
    "    \"\"\"Test loading and parsing of training dataset\"\"\"\n",
    "    df = load_data(test_df_path)\n",
    "    \n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Number of documents: {len(df)}\")\n",
    "    print(\"\\nColumns present:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    # Check first row\n",
    "    first_row = df.iloc[0]\n",
    "    print(\"\\nFirst row contents:\")\n",
    "    print(f\"Document ID: {first_row.get('doc_id')}\")\n",
    "    print(f\"Text length: {len(first_row['note_text'])} characters\")\n",
    "    print(f\"Number of entities: {len(first_row['entities_json'])}\")\n",
    "    print(f\"Number of dates: {len(first_row['dates_json'])}\")\n",
    "    \n",
    "    # Sample of parsed content\n",
    "    print(\"\\nSample entities (first 3):\")\n",
    "    for e in first_row['entities_json'][:3]:\n",
    "        print(f\"- {e['value']} (Position: {e['start']}-{e['end']})\")\n",
    "    \n",
    "    print(\"\\nSample dates (first 3):\")\n",
    "    for d in first_row['dates_json'][:3]:\n",
    "        print(f\"- {d['value']} (Position: {d['start']}-{d['end']})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run test\n",
    "df = test_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test prepare_sample\n",
    "def test_prepare_sample():\n",
    "    \"\"\"Test preparation of a single sample\"\"\"\n",
    "    # Get first row\n",
    "    row = df.iloc[0]\n",
    "    \n",
    "    # Prepare sample\n",
    "    note_text, entities_list, dates = prepare_sample(row)\n",
    "    \n",
    "    print(\"Sample Preparation Results:\")\n",
    "    print(f\"\\nText length: {len(note_text)} characters\")\n",
    "    print(f\"Number of entities: {len(entities_list)}\")\n",
    "    print(f\"Number of dates: {len(dates)}\")\n",
    "    \n",
    "    print(\"\\nFirst 3 entities:\")\n",
    "    for e in entities_list[:3]:\n",
    "        print(f\"- {e['value']} (Position: {e['start']}-{e['end']})\")\n",
    "    \n",
    "    print(\"\\nFirst 3 dates:\")\n",
    "    for d in dates[:3]:\n",
    "        print(f\"- {d['value']} (Position: {d['start']}-{d['end']})\")\n",
    "    \n",
    "    return note_text, entities_list, dates\n",
    "\n",
    "# Run test\n",
    "note_text, entities_list, dates = test_prepare_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test prepare_all_samples\n",
    "def test_prepare_all_samples():\n",
    "    \"\"\"Test preparation of all samples\"\"\"\n",
    "    samples = prepare_all_samples(df)\n",
    "    \n",
    "    print(\"All Samples Preparation Results:\")\n",
    "    print(f\"Number of samples prepared: {len(samples)}\")\n",
    "    \n",
    "    # Check first sample\n",
    "    first_sample = samples[0]\n",
    "    print(\"\\nFirst sample contents:\")\n",
    "    print(f\"- doc_id: {first_sample['doc_id']}\")\n",
    "    print(f\"- Text length: {len(first_sample['note_text'])} characters\")\n",
    "    print(f\"- Number of entities: {len(first_sample['entities_list'])}\")\n",
    "    print(f\"- Number of dates: {len(first_sample['dates'])}\")\n",
    "    print(f\"- Number of relative dates: {len(first_sample['relative_dates'])}\")\n",
    "    \n",
    "    # Print first few entities and dates\n",
    "    print(\"\\nFirst 3 entities:\")\n",
    "    for e in first_sample['entities_list'][:3]:\n",
    "        print(f\"- {e['value']} (Position: {e['start']}-{e['end']})\")\n",
    "    \n",
    "    print(\"\\nFirst 3 dates:\")\n",
    "    for d in first_sample['dates'][:3]:\n",
    "        print(f\"- {d['value']} (Position: {d['start']}-{d['end']})\")\n",
    "    \n",
    "    print(\"\\nFirst 3 relative dates:\")\n",
    "    for rd in first_sample['relative_dates'][:3]:\n",
    "        print(f\"- {rd['value']} (Position: {rd['start']}-{rd['end']})\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Run test\n",
    "samples = test_prepare_all_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test get_entity_date_pairs\n",
    "def test_get_entity_date_pairs():\n",
    "    \"\"\"Test creation of entity-date pairs\"\"\"\n",
    "    # Get first sample\n",
    "    sample = samples[0]\n",
    "    \n",
    "    # Get pairs\n",
    "    pairs = get_entity_date_pairs(\n",
    "        sample['entities_list'],\n",
    "        sample['dates'],\n",
    "        sample['relative_dates']\n",
    "    )\n",
    "    \n",
    "    print(\"Entity-Date Pairs Results:\")\n",
    "    print(f\"Total pairs generated: {len(pairs)}\")\n",
    "    \n",
    "    print(\"\\nFirst 5 pairs:\")\n",
    "    for i, pair in enumerate(pairs[:5]):\n",
    "        print(f\"\\nPair {i+1}:\")\n",
    "        print(f\"Entity: {pair['entity_label']} ({pair['entity']['start']}-{pair['entity']['end']})\")\n",
    "        print(f\"Date: {pair['date']} ({pair['date_info']['start']}-{pair['date_info']['end']})\")\n",
    "        print(f\"Distance: {pair['distance']} chars\")\n",
    "        print(f\"Date type: {pair['date_type']}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Run test\n",
    "pairs = test_get_entity_date_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Relative Date Handling\n",
    "def test_relative_dates():\n",
    "    \"\"\"Test specific handling of relative dates in pairs\"\"\"\n",
    "    \n",
    "    # Get first sample\n",
    "    sample = samples[0]\n",
    "    \n",
    "    print(\"Relative Date Analysis:\")\n",
    "    print(f\"Total relative dates: {len(sample['relative_dates'])}\")\n",
    "    \n",
    "    # Show all relative dates\n",
    "    print(\"\\nAll relative dates:\")\n",
    "    for rd in sample['relative_dates']:\n",
    "        print(f\"- {rd['value']} (Position: {rd['start']}-{rd['end']})\")\n",
    "    \n",
    "    # Find pairs with relative dates\n",
    "    pairs = get_entity_date_pairs(\n",
    "        sample['entities_list'],\n",
    "        sample['dates'],\n",
    "        sample['relative_dates']\n",
    "    )\n",
    "    \n",
    "    relative_pairs = [p for p in pairs if p['date_type'] == 'relative']\n",
    "    print(f\"\\nPairs using relative dates: {len(relative_pairs)}\")\n",
    "    print(\"\\nFirst 3 relative date pairs:\")\n",
    "    for p in relative_pairs[:3]:\n",
    "        print(f\"\\nEntity: {p['entity_label']} ({p['entity']['start']}-{p['entity']['end']})\")\n",
    "        print(f\"Date: {p['date']} ({p['date_info']['start']}-{p['date_info']['end']})\")\n",
    "        print(f\"Distance: {p['distance']} chars\")\n",
    "\n",
    "# Run test\n",
    "test_relative_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Duplicate Relations\n",
    "def test_duplicate_relations(samples):\n",
    "    \"\"\"Investigate potential duplicate relations in training pairs\"\"\"\n",
    "    print(\"=== Duplicate Relations Analysis ===\")\n",
    "    \n",
    "    # Get first sample\n",
    "    sample = samples[0]\n",
    "    \n",
    "    # Analyze relations\n",
    "    print(\"\\nOriginal Relations:\")\n",
    "    relations = sample['relations_json']\n",
    "    print(f\"Total relations in gold set: {len(relations)}\")\n",
    "    \n",
    "    # Check for duplicates in relations\n",
    "    entity_date_pairs = [(r['entity'], r['date']) for r in relations]\n",
    "    unique_pairs = set(entity_date_pairs)\n",
    "    print(f\"Unique entity-date pairs: {len(unique_pairs)}\")\n",
    "    \n",
    "    if len(entity_date_pairs) != len(unique_pairs):\n",
    "        print(\"\\nFound duplicate relations:\")\n",
    "        from collections import Counter\n",
    "        duplicates = Counter(entity_date_pairs)\n",
    "        for pair, count in duplicates.items():\n",
    "            if count > 1:\n",
    "                print(f\"- {pair[0]} -> {pair[1]} (appears {count} times)\")\n",
    "    \n",
    "    # Create training pairs and analyze\n",
    "    df = create_training_pairs([sample])\n",
    "    positive_pairs = df[df['label'] == 1]\n",
    "    \n",
    "    print(\"\\nTraining Pairs Analysis:\")\n",
    "    print(f\"Total pairs created: {len(df)}\")\n",
    "    print(f\"Positive pairs: {len(positive_pairs)}\")\n",
    "    \n",
    "    # Check if same entity-date pair appears multiple times\n",
    "    pair_texts = [(row['marked_text'].split('[E1]')[1].split('[/E1]')[0].strip(),\n",
    "                  row['marked_text'].split('[E2]')[1].split('[/E2]')[0].strip())\n",
    "                  for _, row in positive_pairs.iterrows()]\n",
    "    unique_pair_texts = set(pair_texts)\n",
    "    \n",
    "    print(f\"Unique positive pairs: {len(unique_pair_texts)}\")\n",
    "    \n",
    "    if len(pair_texts) != len(unique_pair_texts):\n",
    "        print(\"\\nFound duplicate pairs in training data:\")\n",
    "        duplicates = Counter(pair_texts)\n",
    "        for pair, count in duplicates.items():\n",
    "            if count > 1:\n",
    "                print(f\"- {pair[0]} -> {pair[1]} (appears {count} times)\")\n",
    "\n",
    "# Run test\n",
    "test_duplicate_relations(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Token Length and Distance Analysis\n",
    "def test_text_lengths_and_distances():\n",
    "    \"\"\"Analyze document lengths and distances between entities/dates\"\"\"\n",
    "    print(\"=== Text Length and Distance Analysis ===\")\n",
    "    \n",
    "    # Document Length Analysis across all samples\n",
    "    print(\"\\nDocument Length Analysis:\")\n",
    "    text_lengths = [len(s['note_text']) for s in samples]\n",
    "    word_counts = [len(s['note_text'].split()) for s in samples]\n",
    "    \n",
    "    print(f\"Total documents: {len(samples)}\")\n",
    "    print(f\"Mean document length: {sum(text_lengths)/len(text_lengths):.1f} characters\")\n",
    "    print(f\"Min document length: {min(text_lengths)} characters\")\n",
    "    print(f\"Max document length: {max(text_lengths)} characters\")\n",
    "    print(f\"Mean words per document: {sum(word_counts)/len(word_counts):.1f} words\")\n",
    "    print(f\"Min words: {min(word_counts)} words\")\n",
    "    print(f\"Max words: {max(word_counts)} words\")\n",
    "    \n",
    "    # Distance Analysis for Relations across all documents\n",
    "    print(\"\\nDistance Analysis for Relations:\")\n",
    "    \n",
    "    all_distances = []\n",
    "    abs_distances = []\n",
    "    rel_distances = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        for relation in sample['relations_json']:\n",
    "            # Find corresponding entity and date objects\n",
    "            entity = next((e for e in sample['entities_list'] if str(e['id']) == str(relation['entity_id'])), None)\n",
    "            date = next((d for d in sample['dates'] if str(d['id']) == str(relation['date_id'])), None)\n",
    "            rel_date = next((rd for rd in sample['relative_dates'] if str(rd['id']) == str(relation['date_id'])), None)\n",
    "            \n",
    "            if entity:\n",
    "                if date:\n",
    "                    distance = abs(entity['start'] - date['start'])\n",
    "                    all_distances.append(distance)\n",
    "                    abs_distances.append(distance)\n",
    "                elif rel_date:\n",
    "                    distance = abs(entity['start'] - rel_date['start'])\n",
    "                    all_distances.append(distance)\n",
    "                    rel_distances.append(distance)\n",
    "    \n",
    "    print(\"\\nAll Relations:\")\n",
    "    print(f\"Total relations: {len(all_distances)}\")\n",
    "    if all_distances:\n",
    "        print(f\"Mean distance: {sum(all_distances)/len(all_distances):.1f} chars\")\n",
    "        print(f\"Min distance: {min(all_distances)} chars\")\n",
    "        print(f\"Max distance: {max(all_distances)} chars\")\n",
    "    \n",
    "    print(\"\\nAbsolute Date Relations:\")\n",
    "    print(f\"Total relations: {len(abs_distances)}\")\n",
    "    if abs_distances:\n",
    "        print(f\"Mean distance: {sum(abs_distances)/len(abs_distances):.1f} chars\")\n",
    "        print(f\"Min distance: {min(abs_distances)} chars\")\n",
    "        print(f\"Max distance: {max(abs_distances)} chars\")\n",
    "    \n",
    "    print(\"\\nRelative Date Relations:\")\n",
    "    print(f\"Total relations: {len(rel_distances)}\")\n",
    "    if rel_distances:\n",
    "        print(f\"Mean distance: {sum(rel_distances)/len(rel_distances):.1f} chars\")\n",
    "        print(f\"Min distance: {min(rel_distances)} chars\")\n",
    "        print(f\"Max distance: {max(rel_distances)} chars\")\n",
    "    \n",
    "    # Show examples of closest and furthest pairs across all documents\n",
    "    if all_distances:\n",
    "        print(\"\\nExample Relations:\")\n",
    "        closest_global = min(all_distances)\n",
    "        furthest_global = max(all_distances)\n",
    "        \n",
    "        # Find examples of closest and furthest pairs\n",
    "        for sample in samples:\n",
    "            for relation in sample['relations_json']:\n",
    "                entity = next((e for e in sample['entities_list'] if str(e['id']) == str(relation['entity_id'])), None)\n",
    "                date = next((d for d in sample['dates'] if str(d['id']) == str(relation['date_id'])), None)\n",
    "                rel_date = next((rd for rd in sample['relative_dates'] if str(rd['id']) == str(relation['date_id'])), None)\n",
    "                \n",
    "                if entity and (date or rel_date):\n",
    "                    date_obj = date if date else rel_date\n",
    "                    distance = abs(entity['start'] - date_obj['start'])\n",
    "                    \n",
    "                    if distance == closest_global:\n",
    "                        print(f\"\\nClosest Pair (across all documents):\")\n",
    "                        print(f\"Distance: {distance} chars\")\n",
    "                        print(f\"Entity: {entity['value']} (Position: {entity['start']}-{entity['end']})\")\n",
    "                        print(f\"Date: {date_obj['value']} (Position: {date_obj['start']}-{date_obj['end']})\")\n",
    "                        print(f\"Document ID: {sample['doc_id']}\")\n",
    "                    \n",
    "                    if distance == furthest_global:\n",
    "                        print(f\"\\nFurthest Pair (across all documents):\")\n",
    "                        print(f\"Distance: {distance} chars\")\n",
    "                        print(f\"Entity: {entity['value']} (Position: {entity['start']}-{entity['end']})\")\n",
    "                        print(f\"Date: {date_obj['value']} (Position: {date_obj['start']}-{date_obj['end']})\")\n",
    "                        print(f\"Document ID: {sample['doc_id']}\")\n",
    "\n",
    "test_text_lengths_and_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Metrics Calculation\n",
    "def test_metrics_calculation(samples):\n",
    "    \"\"\"Compare metrics calculation with position-based vs unique pairs\"\"\"\n",
    "    print(\"=== Metrics Calculation Analysis ===\")\n",
    "    \n",
    "    # Get first sample\n",
    "    sample = samples[0]\n",
    "    \n",
    "    # Create training pairs (position-based)\n",
    "    df = create_training_pairs([sample])\n",
    "    \n",
    "    # Analyze position-based predictions\n",
    "    print(\"\\nPosition-based Training Pairs:\")\n",
    "    print(f\"Total pairs: {len(df)}\")\n",
    "    print(f\"Positive pairs: {len(df[df['label'] == 1])}\")\n",
    "    print(f\"Negative pairs: {len(df[df['label'] == 0])}\")\n",
    "    \n",
    "    # Convert to unique entity-date pairs\n",
    "    unique_pairs = set()\n",
    "    unique_positive_pairs = set()\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # Find all marker positions\n",
    "            e1_start = row['marked_text'].find('[E1]')\n",
    "            e1_end = row['marked_text'].find('[/E1]')\n",
    "            e2_start = row['marked_text'].find('[E2]')\n",
    "            e2_end = row['marked_text'].find('[/E2]')\n",
    "            \n",
    "            # Only process if all markers are found\n",
    "            if all(pos != -1 for pos in [e1_start, e1_end, e2_start, e2_end]):\n",
    "                entity = row['marked_text'][e1_start+4:e1_end].strip()\n",
    "                date = row['marked_text'][e2_start+4:e2_end].strip()\n",
    "                pair = (entity, date)\n",
    "                unique_pairs.add(pair)\n",
    "                if row['label'] == 1:\n",
    "                    unique_positive_pairs.add(pair)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process row due to {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nUnique Entity-Date Pairs:\")\n",
    "    print(f\"Total unique pairs: {len(unique_pairs)}\")\n",
    "    print(f\"Unique positive pairs: {len(unique_positive_pairs)}\")\n",
    "    print(f\"Unique negative pairs: {len(unique_pairs) - len(unique_positive_pairs)}\")\n",
    "    \n",
    "    # Show example of how same pair appears in different positions\n",
    "    if unique_positive_pairs:\n",
    "        print(\"\\nExample of position variations for same pair:\")\n",
    "        example_pair = next(iter(unique_positive_pairs))\n",
    "        positions = []\n",
    "        for _, row in df[df['label'] == 1].iterrows():\n",
    "            try:\n",
    "                entity = row['marked_text'][row['marked_text'].find('[E1]')+4:row['marked_text'].find('[/E1]')].strip()\n",
    "                date = row['marked_text'][row['marked_text'].find('[E2]')+4:row['marked_text'].find('[/E2]')].strip()\n",
    "                if (entity, date) == example_pair:\n",
    "                    positions.append({\n",
    "                        'entity_pos': (row['ent1_start'], row['ent1_end']),\n",
    "                        'date_pos': (row['ent2_start'], row['ent2_end']),\n",
    "                        'distance': row['distance']\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nPositions for pair '{example_pair[0]} -> {example_pair[1]}':\")\n",
    "        for pos in positions:\n",
    "            print(f\"- Entity at {pos['entity_pos']}, Date at {pos['date_pos']}, Distance: {pos['distance']} chars\")\n",
    "    \n",
    "    print(\"\\nMetrics Calculation:\")\n",
    "    \n",
    "    # Position-based predictions\n",
    "    position_predictions = [\n",
    "        {'entity_label': row['marked_text'][row['marked_text'].find('[E1]')+4:row['marked_text'].find('[/E1]')].strip(),\n",
    "         'date': row['marked_text'][row['marked_text'].find('[E2]')+4:row['marked_text'].find('[/E2]')].strip()}\n",
    "        for _, row in df[df['label'] == 1].iterrows()\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nMetrics when using all position-based pairs:\")\n",
    "    metrics = calculate_metrics(position_predictions, pd.DataFrame([sample]))\n",
    "    print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"F1: {metrics['f1']:.3f}\")\n",
    "    print(f\"True Positives: {metrics['tp']}\")\n",
    "    print(f\"False Positives: {metrics['fp']}\")\n",
    "    print(f\"False Negatives: {metrics['fn']}\")\n",
    "    \n",
    "    # Unique pair predictions\n",
    "    unique_predictions = [\n",
    "        {'entity_label': entity, 'date': date}\n",
    "        for entity, date in unique_positive_pairs\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nMetrics when using unique pairs:\")\n",
    "    metrics = calculate_metrics(unique_predictions, pd.DataFrame([sample]))\n",
    "    print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"F1: {metrics['f1']:.3f}\")\n",
    "    print(f\"True Positives: {metrics['tp']}\")\n",
    "    print(f\"False Positives: {metrics['fp']}\")\n",
    "    print(f\"False Negatives: {metrics['fn']}\")\n",
    "\n",
    "# Run test\n",
    "test_metrics_calculation(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe47578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Dataset Statistics\n",
    "def test_dataset_statistics(samples):\n",
    "    \"\"\"Compare dataset statistics across different approaches\"\"\"\n",
    "    print(\"=== Dataset Statistics Comparison ===\")\n",
    "    \n",
    "    print(\"\\n=== Single Document Analysis ===\")\n",
    "    sample = samples[0]\n",
    "    \n",
    "    print(\"\\n1. Create Training Dataset Approach:\")\n",
    "    n_entities = len(sample['entities_list'])\n",
    "    n_abs_dates = len(sample['dates'])\n",
    "    n_rel_dates = len(sample['relative_dates'])\n",
    "    n_relations = len(sample['relations_json'])\n",
    "    total_possible = n_entities * (n_abs_dates + n_rel_dates)\n",
    "    \n",
    "    print(f\"Number of entities: {n_entities}\")\n",
    "    print(f\"Number of absolute dates: {n_abs_dates}\")\n",
    "    print(f\"Number of relative dates: {n_rel_dates}\")\n",
    "    print(f\"Total possible unique pairs: {total_possible}\")\n",
    "    print(f\"Number of actual relations: {n_relations}\")\n",
    "    print(f\"Percentage positive class: {(n_relations / total_possible) * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n2. BERT Training Approach:\")\n",
    "    df = create_training_pairs([sample])\n",
    "    \n",
    "    print(\"\\nPosition-based pairs:\")\n",
    "    print(f\"Total pairs: {len(df)}\")\n",
    "    print(f\"Positive pairs: {len(df[df['label'] == 1])}\")\n",
    "    print(f\"Negative pairs: {len(df[df['label'] == 0])}\")\n",
    "    print(f\"Percentage positive: {(len(df[df['label'] == 1]) / len(df) * 100):.2f}%\")\n",
    "    \n",
    "    unique_pairs = set()\n",
    "    unique_positive_pairs = set()\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            e1_start = row['marked_text'].find('[E1]')\n",
    "            e1_end = row['marked_text'].find('[/E1]')\n",
    "            e2_start = row['marked_text'].find('[E2]')\n",
    "            e2_end = row['marked_text'].find('[/E2]')\n",
    "            \n",
    "            if all(pos != -1 for pos in [e1_start, e1_end, e2_start, e2_end]):\n",
    "                entity = row['marked_text'][e1_start+4:e1_end].strip()\n",
    "                date = row['marked_text'][e2_start+4:e2_end].strip()\n",
    "                pair = (entity, date)\n",
    "                unique_pairs.add(pair)\n",
    "                if row['label'] == 1:\n",
    "                    unique_positive_pairs.add(pair)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nUnique pairs:\")\n",
    "    print(f\"Total unique pairs: {len(unique_pairs)}\")\n",
    "    print(f\"Unique positive pairs: {len(unique_positive_pairs)}\")\n",
    "    print(f\"Unique negative pairs: {len(unique_pairs) - len(unique_positive_pairs)}\")\n",
    "    print(f\"Percentage positive: {(len(unique_positive_pairs) / len(unique_pairs) * 100):.2f}%\")\n",
    "    \n",
    "    print(\"\\n3. Naive Approach:\")\n",
    "    all_dates = sample['dates'] + sample['relative_dates']\n",
    "    naive_pairs = naive_extraction(sample['entities_list'], all_dates, max_distance=25)\n",
    "    print(f\"Total pairs predicted: {len(naive_pairs)}\")\n",
    "    print(f\"Total possible pairs: {len(sample['entities_list']) * len(all_dates)}\")\n",
    "    \n",
    "    print(\"\\n4. RelCAT Approach:\")\n",
    "    relcat_total = n_entities * (n_abs_dates + n_rel_dates)\n",
    "    print(f\"Total possible pairs: {relcat_total}\")\n",
    "    \n",
    "    print(\"\\n5. LLM Approach:\")\n",
    "    print(\"\\nBinary method:\")\n",
    "    pairs = get_entity_date_pairs(sample['entities_list'], sample['dates'], sample['relative_dates'])\n",
    "    print(f\"Total pairs to evaluate: {len(pairs)}\")\n",
    "    print(f\"Total possible pairs: {n_entities * (n_abs_dates + n_rel_dates)}\")\n",
    "    \n",
    "    print(\"\\nMulti method:\")\n",
    "    print(f\"Total possible pairs: {n_entities * (n_abs_dates + n_rel_dates)}\")\n",
    "    \n",
    "    print(\"\\n=== Full Dataset Analysis ===\")\n",
    "    \n",
    "    print(\"\\n1. Create Training Dataset Approach:\")\n",
    "    total_entities = sum(len(s['entities_list']) for s in samples)\n",
    "    total_abs_dates = sum(len(s['dates']) for s in samples)\n",
    "    total_rel_dates = sum(len(s['relative_dates']) for s in samples)\n",
    "    total_relations = sum(len(s['relations_json']) for s in samples)\n",
    "    total_possible_pairs = sum(len(s['entities_list']) * (len(s['dates']) + len(s['relative_dates'])) for s in samples)\n",
    "    \n",
    "    print(f\"Total entities across all documents: {total_entities}\")\n",
    "    print(f\"Total absolute dates: {total_abs_dates}\")\n",
    "    print(f\"Total relative dates: {total_rel_dates}\")\n",
    "    print(f\"Total possible pairs: {total_possible_pairs}\")\n",
    "    print(f\"Total relations: {total_relations}\")\n",
    "    print(f\"Percentage positive class: {(total_relations / total_possible_pairs) * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n2. BERT Training Approach:\")\n",
    "    df_full = create_training_pairs(samples)\n",
    "    \n",
    "    print(\"\\nPosition-based pairs:\")\n",
    "    print(f\"Total pairs: {len(df_full)}\")\n",
    "    print(f\"Positive pairs: {len(df_full[df_full['label'] == 1])}\")\n",
    "    print(f\"Negative pairs: {len(df_full[df_full['label'] == 0])}\")\n",
    "    print(f\"Percentage positive: {(len(df_full[df_full['label'] == 1]) / len(df_full) * 100):.2f}%\")\n",
    "    \n",
    "    unique_pairs_full = set()\n",
    "    unique_positive_pairs_full = set()\n",
    "    for _, row in df_full.iterrows():\n",
    "        try:\n",
    "            e1_start = row['marked_text'].find('[E1]')\n",
    "            e1_end = row['marked_text'].find('[/E1]')\n",
    "            e2_start = row['marked_text'].find('[E2]')\n",
    "            e2_end = row['marked_text'].find('[/E2]')\n",
    "            \n",
    "            if all(pos != -1 for pos in [e1_start, e1_end, e2_start, e2_end]):\n",
    "                entity = row['marked_text'][e1_start+4:e1_end].strip()\n",
    "                date = row['marked_text'][e2_start+4:e2_end].strip()\n",
    "                pair = (entity, date)\n",
    "                unique_pairs_full.add(pair)\n",
    "                if row['label'] == 1:\n",
    "                    unique_positive_pairs_full.add(pair)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nUnique pairs:\")\n",
    "    print(f\"Total unique pairs: {len(unique_pairs_full)}\")\n",
    "    print(f\"Unique positive pairs: {len(unique_positive_pairs_full)}\")\n",
    "    print(f\"Unique negative pairs: {len(unique_pairs_full) - len(unique_positive_pairs_full)}\")\n",
    "    print(f\"Percentage positive: {(len(unique_positive_pairs_full) / len(unique_pairs_full) * 100):.2f}%\")\n",
    "    \n",
    "    print(\"\\n3. Naive Approach:\")\n",
    "    naive_pairs_full = []\n",
    "    for s in samples:\n",
    "        all_dates = s['dates'] + s['relative_dates']\n",
    "        pairs = naive_extraction(s['entities_list'], all_dates, max_distance=25)\n",
    "        naive_pairs_full.extend(pairs)\n",
    "    print(f\"Total pairs predicted: {len(naive_pairs_full)}\")\n",
    "    print(f\"Total possible pairs: {total_possible_pairs}\")\n",
    "    \n",
    "    print(\"\\n4. RelCAT Approach:\")\n",
    "    relcat_total = total_possible_pairs\n",
    "    print(f\"Total possible pairs: {relcat_total}\")\n",
    "    \n",
    "    print(\"\\n5. LLM Approach:\")\n",
    "    print(\"\\nBinary method:\")\n",
    "    llm_pairs_full = []\n",
    "    for s in samples:\n",
    "        pairs = get_entity_date_pairs(s['entities_list'], s['dates'], s['relative_dates'])\n",
    "        llm_pairs_full.extend(pairs)\n",
    "    print(f\"Total pairs to evaluate: {len(llm_pairs_full)}\")\n",
    "    print(f\"Total possible pairs: {total_possible_pairs}\")\n",
    "    \n",
    "    print(\"\\nMulti method:\")\n",
    "    print(f\"Total possible pairs: {total_possible_pairs}\")\n",
    "\n",
    "test_dataset_statistics(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Test Entity Distribution Analysis\n",
    "def test_entity_distribution():\n",
    "    \"\"\"Analyze the distribution and frequency of entities across all documents\"\"\"\n",
    "    print(\"=== Entity Distribution Analysis ===\")\n",
    "    \n",
    "    # Define CUIs to exclude (date-related)\n",
    "    DATE_CUIS = {'118578006', '410671006', '410670007'}\n",
    "    \n",
    "    # Collect all entities across documents, grouped by CUI\n",
    "    cui_entities = {}\n",
    "    for sample in samples:\n",
    "        for entity in sample['entities_list']:\n",
    "            cui = entity.get('cui', 'N/A')\n",
    "            if cui in DATE_CUIS:  # Skip date-related CUIs\n",
    "                continue\n",
    "            if cui not in cui_entities:\n",
    "                cui_entities[cui] = {\n",
    "                    'mentions': 0,\n",
    "                    'values': set(),  # Track unique text representations\n",
    "                    'examples': []    # Keep some examples of the text\n",
    "                }\n",
    "            cui_entities[cui]['mentions'] += 1\n",
    "            cui_entities[cui]['values'].add(entity['value'])\n",
    "            if len(cui_entities[cui]['examples']) < 3:  # Keep up to 3 examples\n",
    "                if entity['value'] not in cui_entities[cui]['examples']:\n",
    "                    cui_entities[cui]['examples'].append(entity['value'])\n",
    "    \n",
    "    # Sort by mention count\n",
    "    sorted_cuis = sorted(cui_entities.items(), key=lambda x: x[1]['mentions'], reverse=True)\n",
    "    \n",
    "    # Print summary stats\n",
    "    all_mentions = sum(data['mentions'] for data in cui_entities.values())\n",
    "    print(f\"Total CUIs: {len(cui_entities)}\")\n",
    "    print(f\"Total mentions: {all_mentions}\")\n",
    "    print(f\"Average mentions per CUI: {all_mentions/len(cui_entities):.1f}\")\n",
    "    \n",
    "    # Print table in tab-separated format for easy copying to Word\n",
    "    print(\"\\nCopy the following table (including header row) and paste into Word:\\n\")\n",
    "    print(\"Rank\\tCount\\tCUI\\tExample terms\")\n",
    "    for rank, (cui, data) in enumerate(sorted_cuis[:20], 1):\n",
    "        examples = ', '.join(data['examples'])\n",
    "        print(f\"{rank}\\t{data['mentions']}\\t{cui}\\t{examples}\")\n",
    "\n",
    "test_entity_distribution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
