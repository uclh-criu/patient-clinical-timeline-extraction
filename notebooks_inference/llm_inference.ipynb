{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb4652a",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from general_utils import load_data, prepare_all_samples, get_entity_date_pairs\n",
    "\n",
    "from llm_extractor_utils import (\n",
    "    make_binary_prompt, \n",
    "    llm_extraction_binary_hf,\n",
    "    llm_extraction_binary_openai,\n",
    "    parse_llm_answer,\n",
    "    make_multi_prompt,\n",
    "    llm_extraction_multi_openai,\n",
    "    llm_extraction_multi_hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368136f",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_data(\"../data/inference_dataset_synthetic1.csv\")\n",
    "print(f\"Loaded {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d08fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all samples\n",
    "samples = prepare_all_samples(df)\n",
    "print(f\"Prepared {len(samples)} samples\")\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc438d4a",
   "metadata": {},
   "source": [
    "LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose whether to use HuggingFace 'hf' or OpenAI 'openai'\n",
    "#Note if choosing OpenAI need to set OPENAI_API_KEY in .env file\n",
    "provider = 'openai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device\n",
    "device = -1  # Set to -1 for CPU, 0 for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose whether to do binary prediction 'binary' or multi-prediction 'multi'\n",
    "#Binary prediction is where the LLM is passed every possible entity-date pair and asked to predict whether they are related (Yes or No). This requires a lot more LLM calls so can be expensive if using the OpenAI API\n",
    "#Multi prediction is where the LLM is passed each note in turn and asked to extract all correct relations in one go as a json\n",
    "method = 'multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose whether to use zero-shot 'zero' or few-shot prompt 'few'\n",
    "prompt_type = 'few'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02166ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see a list of available OpenAI API models\n",
    "#Alternatively visit https://platform.openai.com/docs/models\n",
    "\n",
    "#client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))\n",
    "#models = client.models.list()\n",
    "#for model in models:\n",
    "    #print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76809e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see a list of available HuggingFace text generation models\n",
    "#Alternatively visit https://huggingface.co/models?pipeline_tag=text-generation\n",
    "\n",
    "#Uncomment the below code to see the full list. Note this list is long and can take a couple of mins to run\n",
    "#api = HfApi()\n",
    "#text_gen_models = api.list_models(filter=\"text-generation\")\n",
    "#for model_info in text_gen_models:\n",
    "    #print(model_info.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM to use - this should be the model id from OpenAI or HuggingFace\n",
    "model = 'gpt-3.5-turbo'\n",
    "\n",
    "#Examples of OpenAI models\n",
    "#model = 'gpt-3.5-turbo' #cheap option for binary method\n",
    "#model = 'o4-mini'\n",
    "#model = 'gpt-5-mini' #best reasoning option\n",
    "\n",
    "#Examples of HF models\n",
    "#model = 'gpt2'\n",
    "#model = 'google/gemma-3-270m'\n",
    "#model= 'Qwen/Qwen3-0.6B'\n",
    "#model= 'roneneldan/TinyStories-1M' #fast option even on CPU, good for binary method\n",
    "#model = 'EleutherAI/gpt-neo-125m'\n",
    "#model = 'microsoft/phi-1_5'\n",
    "#model = 'TinyLlama/TinyLlama-1.1B-chat-v1.0'\n",
    "#model = '../Llama-3.1-8B-Instruct' #lcoal folder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure quantization if using HF opion\n",
    "if provider == 'hf':\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81853642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator if using HF option\n",
    "if provider == 'hf':\n",
    "    if method == 'multi':\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            #device=device,\n",
    "            model_kwargs={\"quantization_config\": quantization_config},\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=False,\n",
    "            #pad_token_id=2,  # Common pad token ID for Llama models\n",
    "            #eos_token_id=2,  # End of sequence token\n",
    "            return_full_text=False  # Only return the newly generated text\n",
    "        )\n",
    "    else:\n",
    "        # Regular text generation for binary method\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            #device=device,\n",
    "            model_kwargs={\"quantization_config\": quantization_config}\n",
    "        )\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to use\n",
    "if method == 'binary' and prompt_type == 'zero':\n",
    "    prompt_to_use = 'binary_prompt_zero_shot.txt'\n",
    "elif method == 'binary' and prompt_type == 'few':\n",
    "    prompt_to_use = 'binary_prompt_few_shot.txt'\n",
    "elif method == 'multi' and prompt_type == 'zero':\n",
    "    prompt_to_use = 'multi_prompt_zero_shot.txt'\n",
    "elif method == 'multi' and prompt_type == 'few':\n",
    "    prompt_to_use = 'multi_prompt_few_shot.txt'\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Invalid method: {method} or prompt type {prompt_type}. \"\n",
    "        \"Method must be either 'binary' or 'multi' and prompt type must be either 'zero' or 'few'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process samples, make prompt, do llm extraction and make predictions\n",
    "predictions = []\n",
    "\n",
    "if method == 'binary':\n",
    "    for sample in tqdm(samples, desc=\"Samples\"):\n",
    "        \n",
    "        # Get absolute date pairs\n",
    "        absolute_pairs = get_entity_date_pairs(sample['entities_list'], sample['dates'])\n",
    "        \n",
    "        # Get relative date pairs if available\n",
    "        if sample.get('relative_dates') and len(sample['relative_dates']) > 0:\n",
    "            relative_pairs = get_entity_date_pairs(sample['entities_list'], [], sample['relative_dates'])\n",
    "            pairs = absolute_pairs + relative_pairs\n",
    "        else:\n",
    "            pairs = absolute_pairs\n",
    "        \n",
    "        for pair in pairs:\n",
    "            #Create binary prompt\n",
    "            prompt = make_binary_prompt(pair['entity'], pair['date_info'], sample['note_text'], prompt_to_use)\n",
    "            \n",
    "            #Get response based on the method chosen\n",
    "            if provider == 'openai':\n",
    "                response = llm_extraction_binary_openai(prompt, model=model)\n",
    "            elif provider == 'hf':\n",
    "                response = llm_extraction_binary_hf(prompt, generator)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid provider: {provider}. Must be either 'openai' or 'hf'.\")\n",
    "            \n",
    "            #Parse response into binary prediction\n",
    "            pred, conf = parse_llm_answer(response)\n",
    "            if pred == 1:\n",
    "                predictions.append({\n",
    "                    'doc_id': sample['doc_id'],\n",
    "                    'date_id': pair['date_info']['id'],\n",
    "                    'date': pair['date_info']['value'],\n",
    "                    'date_type': pair['date_type'],\n",
    "                    'entity_cui': pair['entity']['cui'],\n",
    "                    'entity_label': pair['entity']['value'],\n",
    "                    'entity_preferred_name': pair['entity'].get('preferred_name', pair['entity']['value']),\n",
    "                    'confidence': conf\n",
    "                })\n",
    "\n",
    "elif method == 'multi':\n",
    "    for sample in tqdm(samples, desc=\"Processing notes\"):\n",
    "        \n",
    "        # Create multi-extraction prompt\n",
    "        prompt = make_multi_prompt(\n",
    "            note_text=sample['note_text'],\n",
    "            prompt_filename=prompt_to_use,\n",
    "            entities_list=sample['entities_list'],\n",
    "            dates=sample['dates']\n",
    "        )\n",
    "        \n",
    "        # Get all relationships in one call\n",
    "        if provider == 'openai':\n",
    "            relationships = llm_extraction_multi_openai(prompt, model=model)\n",
    "        elif provider == 'hf':\n",
    "            relationships = llm_extraction_multi_hf(prompt, generator)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid provider: {provider}. Must be either 'openai' or 'hf'.\")\n",
    "        \n",
    "        # Create lookup maps to enrich the LLM's response\n",
    "        entity_map = {entity['value'].lower(): (entity.get('cui'), entity.get('preferred_name', entity['value'])) for entity in sample['entities_list']}\n",
    "        date_map = {date['value']: (date['id'], 'absolute') for date in sample['dates']}\n",
    "        date_map.update({date['value']: (date['id'], 'relative') for date in sample.get('relative_dates', [])})\n",
    "        \n",
    "        # Post-process and re-order relationships\n",
    "        ordered_predictions = []\n",
    "        for rel in relationships:\n",
    "            entity_label = rel.get('entity_label', '')\n",
    "            date_value = rel.get('date', '')\n",
    "            \n",
    "            entity_cui, preferred_name = entity_map.get(entity_label.lower(), (None, entity_label))\n",
    "            date_id, date_type = date_map.get(date_value, (None, 'unknown'))\n",
    "            \n",
    "            ordered_predictions.append({\n",
    "                'doc_id': sample['doc_id'],\n",
    "                'date_id': date_id,\n",
    "                'date': date_value,\n",
    "                'date_type': date_type,\n",
    "                'entity_cui': entity_cui,\n",
    "                'entity_label': entity_label,\n",
    "                'entity_preferred_name': preferred_name\n",
    "            })\n",
    "            \n",
    "        predictions.extend(ordered_predictions)\n",
    "\n",
    "print(f\"Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51463dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at prediction\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "with open('../outputs/llm_predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(\"Saved predictions to outputs/llm_predictions.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
