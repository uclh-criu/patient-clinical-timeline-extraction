{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b2cc81",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17372f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config_rel_cat import ConfigRelCAT\n",
    "from medcat.rel_cat import RelCAT\n",
    "\n",
    "import sys, os\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from general_utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0cbab",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bdbf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 101 records\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = load_data(\"../data/inference_dataset.csv\")\n",
    "print(f\"Loaded {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd82f857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>note_text</th>\n",
       "      <th>entities_json</th>\n",
       "      <th>dates_json</th>\n",
       "      <th>relative_dates_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ultrasound (30nd Jun 2024): no significant fin...</td>\n",
       "      <td>[{'id': 'ent_1', 'value': 'Ultrasound', 'cui':...</td>\n",
       "      <td>[{'id': 'abs_1', 'value': '30nd Jun 2024', 'st...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Labs (27th Sep 2024): anemia. resolving  Skin:...</td>\n",
       "      <td>[{'id': 'ent_1', 'value': 'anemia', 'cui': 'C0...</td>\n",
       "      <td>[{'id': 'abs_1', 'value': '27th Sep 2024', 'st...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>URGENT REVIEW (2024-10-04): cough. suspect ost...</td>\n",
       "      <td>[{'id': 'ent_1', 'value': 'REVIEW', 'cui': 'C1...</td>\n",
       "      <td>[{'id': 'abs_1', 'value': '2024-10-04', 'start...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>URGENT REVIEW (13rd Feb 2025) MRI of the brain...</td>\n",
       "      <td>[{'id': 'ent_1', 'value': 'REVIEW', 'cui': 'C0...</td>\n",
       "      <td>[{'id': 'abs_1', 'value': '13rd Feb 2025', 'st...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New pt((18/11/24)): pt presents with nausea/vo...</td>\n",
       "      <td>[{'id': 'ent_1', 'value': 'nausea', 'cui': 'C0...</td>\n",
       "      <td>[{'id': 'abs_1', 'value': '18/11/24', 'start':...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                          note_text  \\\n",
       "0       0  Ultrasound (30nd Jun 2024): no significant fin...   \n",
       "1       1  Labs (27th Sep 2024): anemia. resolving  Skin:...   \n",
       "2       2  URGENT REVIEW (2024-10-04): cough. suspect ost...   \n",
       "3       3  URGENT REVIEW (13rd Feb 2025) MRI of the brain...   \n",
       "4       4  New pt((18/11/24)): pt presents with nausea/vo...   \n",
       "\n",
       "                                       entities_json  \\\n",
       "0  [{'id': 'ent_1', 'value': 'Ultrasound', 'cui':...   \n",
       "1  [{'id': 'ent_1', 'value': 'anemia', 'cui': 'C0...   \n",
       "2  [{'id': 'ent_1', 'value': 'REVIEW', 'cui': 'C1...   \n",
       "3  [{'id': 'ent_1', 'value': 'REVIEW', 'cui': 'C0...   \n",
       "4  [{'id': 'ent_1', 'value': 'nausea', 'cui': 'C0...   \n",
       "\n",
       "                                          dates_json relative_dates_json  \n",
       "0  [{'id': 'abs_1', 'value': '30nd Jun 2024', 'st...                  []  \n",
       "1  [{'id': 'abs_1', 'value': '27th Sep 2024', 'st...                  []  \n",
       "2  [{'id': 'abs_1', 'value': '2024-10-04', 'start...                  []  \n",
       "3  [{'id': 'abs_1', 'value': '13rd Feb 2025', 'st...                  []  \n",
       "4  [{'id': 'abs_1', 'value': '18/11/24', 'start':...                  []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332aa3d6",
   "metadata": {},
   "source": [
    "RelCAT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2d6c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:medcat.utils.relation_extraction.base_component:BaseComponent_RelationExtraction initialized\n",
      "INFO:medcat.utils.relation_extraction.base_component:BaseComponent_RelationExtraction initialized\n"
     ]
    }
   ],
   "source": [
    "#Load trained RelCAT model\n",
    "relCAT = RelCAT.load(\"../models/relcat_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603e9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cuis for absolute and relative dates (these should align with the cuis that were used to add these terms in MedCAT Trainer)\n",
    "DATE_CUI = \"410671006\"\n",
    "RELATIVE_DATE_CUI = \"410671007\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06194e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing document 0: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:medcat.rel_cat:total relations for doc: 346\n",
      "INFO:medcat.rel_cat:processing...\n",
      "  0%|          | 0/346 [00:00<?, ?it/s]c:\\Users\\Satyam\\Downloads\\pituitary_adenoma\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "\n",
    "doc_ids = df['doc_id'].unique()\n",
    "\n",
    "for doc_id in doc_ids:\n",
    "    # Get the row\n",
    "    row = df[df['doc_id'] == doc_id].iloc[0]\n",
    "    \n",
    "    # Parse the JSON columns\n",
    "    entities = row[\"entities_json\"] if isinstance(row[\"entities_json\"], list) else json.loads(row[\"entities_json\"])\n",
    "    dates = row[\"dates_json\"] if isinstance(row[\"dates_json\"], list) else json.loads(row[\"dates_json\"])\n",
    "    relative_dates = row[\"relative_dates_json\"] if isinstance(row[\"relative_dates_json\"], list) else json.loads(row[\"relative_dates_json\"]) if \"relative_dates_json\" in row else []\n",
    "    \n",
    "    # Combine absolute and relative dates\n",
    "    all_dates = dates + relative_dates\n",
    "    \n",
    "    # Create annotations in the same format as training\n",
    "    annotations = []\n",
    "    for entity in entities:\n",
    "        annotations.append({\n",
    "            \"value\": entity[\"value\"],\n",
    "            \"cui\": entity.get(\"cui\"),\n",
    "            \"start\": entity.get(\"start\"),\n",
    "            \"end\": entity.get(\"end\")\n",
    "        })\n",
    "    for date in all_dates:\n",
    "        annotations.append({\n",
    "            \"value\": date[\"value\"],\n",
    "            \"cui\": DATE_CUI,\n",
    "            \"start\": date.get(\"start\"),\n",
    "            \"end\": date.get(\"end\")\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        # Run inference\n",
    "        output_doc_with_relations = relCAT.predict_text_with_anns(\n",
    "            text=row[\"note_text\"], \n",
    "            annotations=annotations\n",
    "        )\n",
    "        \n",
    "        # Collect results - only keep date-entity pairs\n",
    "        for relation in output_doc_with_relations._.relations:\n",
    "            # Check if this is a date-entity pair (not entity-entity)\n",
    "            if (relation['ent1_text'] in [d['value'] for d in dates] and \n",
    "                relation['ent2_text'] in [e['value'] for e in entities]) or \\\n",
    "               (relation['ent2_text'] in [d['value'] for d in dates] and \n",
    "                relation['ent1_text'] in [e['value'] for e in entities]):\n",
    "                \n",
    "                all_predictions.append({\n",
    "                    'entity_label': relation['ent1_text'],\n",
    "                    'date': relation['ent2_text'],\n",
    "                    'confidence': relation['confidence'],\n",
    "                    'doc_id': doc_id\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {doc_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Processed {len(doc_ids)} documents\")\n",
    "print(f\"Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659000d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at predictions\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "print(\"Test Set Results:\")\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "# Show first 10 predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "for i, pred in enumerate(all_predictions[:10]):\n",
    "    print(f\"{i+1}. {pred['entity_label']} -> {pred['date']} (conf: {pred['confidence']:.3f}) [doc: {pred['doc_id']}]\")\n",
    "\n",
    "# Show high confidence predictions\n",
    "high_conf = [p for p in all_predictions if p['confidence'] > 0.7]\n",
    "print(f\"\\nHigh confidence predictions (>0.7): {len(high_conf)}\")\n",
    "for i, pred in enumerate(high_conf[:5]):  # Show first 5\n",
    "    print(f\"{i+1}. {pred['entity_label']} -> {pred['date']} (conf: {pred['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's debug the exact counts\n",
    "print(f\"Total test pairs: {len(df)}\")\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "# Count how many test pairs were actually predicted\n",
    "predicted_count = 0\n",
    "for _, row in df.iterrows():\n",
    "    found = False\n",
    "    for pred in all_predictions:\n",
    "        if (pred['doc_id'] == row['doc_id'] and \n",
    "            ((pred['entity_label'] == row['ent1'] and pred['date'] == row['ent2']) or\n",
    "             (pred['entity_label'] == row['ent2'] and pred['date'] == row['ent1']))):\n",
    "            found = True\n",
    "            break\n",
    "    if found:\n",
    "        predicted_count += 1\n",
    "\n",
    "print(f\"Test pairs that were predicted: {predicted_count}\")\n",
    "print(f\"Test pairs that were NOT predicted: {len(df) - predicted_count}\")\n",
    "\n",
    "# Also check if there are predictions for documents not in test set\n",
    "test_doc_ids = set(df['doc_id'].unique())\n",
    "pred_doc_ids = set([p['doc_id'] for p in all_predictions])\n",
    "print(f\"Test doc IDs: {test_doc_ids}\")\n",
    "print(f\"Prediction doc IDs: {pred_doc_ids}\")\n",
    "print(f\"Extra predictions (not in test): {len(pred_doc_ids - test_doc_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for all test pairs\n",
    "all_test_predictions = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Check if this pair was predicted as a link\n",
    "    found = False\n",
    "    for pred in all_predictions:\n",
    "        if (pred['doc_id'] == row['doc_id'] and \n",
    "            ((pred['entity_label'] == row['ent1'] and pred['date'] == row['ent2']) or\n",
    "             (pred['entity_label'] == row['ent2'] and pred['date'] == row['ent1']))):\n",
    "            all_test_predictions.append('LINK')\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        all_test_predictions.append('NO_LINK')\n",
    "\n",
    "# Now calculate metrics on all test pairs\n",
    "y_true_all = df['label'].tolist()\n",
    "y_pred_all = all_test_predictions\n",
    "\n",
    "print(f\"\\nAll Test Pairs Metrics:\")\n",
    "print(f\"Accuracy: {sum(1 for t, p in zip(y_true_all, y_pred_all) if t == p) / len(y_true_all):.3f}\")\n",
    "print(classification_report(y_true_all, y_pred_all, labels=['LINK', 'NO_LINK']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
