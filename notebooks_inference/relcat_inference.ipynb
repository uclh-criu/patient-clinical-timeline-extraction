{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b2cc81",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17372f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config_rel_cat import ConfigRelCAT\n",
    "from medcat.rel_cat import RelCAT\n",
    "\n",
    "import sys, os\n",
    "utils_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from general_utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0cbab",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bdbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_data(\"../data/inference_dataset.csv\")\n",
    "print(f\"Loaded {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332aa3d6",
   "metadata": {},
   "source": [
    "RelCAT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained RelCAT model\n",
    "relCAT = RelCAT.load(\"../models/relcat_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cuis for absolute and relative dates (these should align with the cuis that were used to add these terms in MedCAT Trainer)\n",
    "DATE_CUI = \"410671006\"\n",
    "RELATIVE_DATE_CUI = \"118578006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06194e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = []\n",
    "\n",
    "doc_ids = df['doc_id'].unique()\n",
    "\n",
    "for doc_id in doc_ids:\n",
    "    # Get the row\n",
    "    row = df[df['doc_id'] == doc_id].iloc[0]\n",
    "    \n",
    "    # Parse the JSON columns\n",
    "    entities = row[\"entities_json\"] if isinstance(row[\"entities_json\"], list) else json.loads(row[\"entities_json\"])\n",
    "    dates = row[\"dates_json\"] if isinstance(row[\"dates_json\"], list) else json.loads(row[\"dates_json\"])\n",
    "    relative_dates = row[\"relative_dates_json\"] if isinstance(row[\"relative_dates_json\"], list) else json.loads(row[\"relative_dates_json\"]) if \"relative_dates_json\" in row else []\n",
    "    \n",
    "    # Combine absolute and relative dates\n",
    "    all_dates = dates + relative_dates\n",
    "    \n",
    "    # Create annotations in the same format as training\n",
    "    annotations = []\n",
    "    for entity in entities:\n",
    "        annotations.append({\n",
    "            \"value\": entity[\"value\"],\n",
    "            \"cui\": entity.get(\"cui\"),\n",
    "            \"start\": entity.get(\"start\"),\n",
    "            \"end\": entity.get(\"end\")\n",
    "        })\n",
    "    for date in all_dates:\n",
    "        annotations.append({\n",
    "            \"value\": date[\"value\"],\n",
    "            \"cui\": DATE_CUI,\n",
    "            \"start\": date.get(\"start\"),\n",
    "            \"end\": date.get(\"end\")\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        # Run inference\n",
    "        output_doc_with_relations = relCAT.predict_text_with_anns(\n",
    "            text=row[\"note_text\"], \n",
    "            annotations=annotations\n",
    "        )\n",
    "        \n",
    "        # Collect results - only keep date-entity pairs\n",
    "        for relation in output_doc_with_relations._.relations:\n",
    "            # Check if this is a date-entity pair (not entity-entity)\n",
    "            if (relation['ent1_text'] in [d['value'] for d in dates] and \n",
    "                relation['ent2_text'] in [e['value'] for e in entities]) or \\\n",
    "               (relation['ent2_text'] in [d['value'] for d in dates] and \n",
    "                relation['ent1_text'] in [e['value'] for e in entities]):\n",
    "                \n",
    "                all_predictions.append({\n",
    "                    'entity_label': relation['ent1_text'],\n",
    "                    'date': relation['ent2_text'],\n",
    "                    'confidence': relation['confidence'],\n",
    "                    'doc_id': doc_id\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {doc_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Processed {len(doc_ids)} documents\")\n",
    "print(f\"Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659000d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at predictions\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "print(\"Test Set Results:\")\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "# Show first 10 predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "for i, pred in enumerate(all_predictions[:10]):\n",
    "    print(f\"{i+1}. {pred['entity_label']} -> {pred['date']} (conf: {pred['confidence']:.3f}) [doc: {pred['doc_id']}]\")\n",
    "\n",
    "# Show high confidence predictions\n",
    "high_conf = [p for p in all_predictions if p['confidence'] > 0.7]\n",
    "print(f\"\\nHigh confidence predictions (>0.7): {len(high_conf)}\")\n",
    "for i, pred in enumerate(high_conf[:5]):  # Show first 5\n",
    "    print(f\"{i+1}. {pred['entity_label']} -> {pred['date']} (conf: {pred['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's debug the exact counts\n",
    "print(f\"Total test pairs: {len(df)}\")\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "# Count how many test pairs were actually predicted\n",
    "predicted_count = 0\n",
    "for _, row in df.iterrows():\n",
    "    found = False\n",
    "    for pred in all_predictions:\n",
    "        if (pred['doc_id'] == row['doc_id'] and \n",
    "            ((pred['entity_label'] == row['ent1'] and pred['date'] == row['ent2']) or\n",
    "             (pred['entity_label'] == row['ent2'] and pred['date'] == row['ent1']))):\n",
    "            found = True\n",
    "            break\n",
    "    if found:\n",
    "        predicted_count += 1\n",
    "\n",
    "print(f\"Test pairs that were predicted: {predicted_count}\")\n",
    "print(f\"Test pairs that were NOT predicted: {len(df) - predicted_count}\")\n",
    "\n",
    "# Also check if there are predictions for documents not in test set\n",
    "test_doc_ids = set(df['doc_id'].unique())\n",
    "pred_doc_ids = set([p['doc_id'] for p in all_predictions])\n",
    "print(f\"Test doc IDs: {test_doc_ids}\")\n",
    "print(f\"Prediction doc IDs: {pred_doc_ids}\")\n",
    "print(f\"Extra predictions (not in test): {len(pred_doc_ids - test_doc_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for all test pairs\n",
    "all_test_predictions = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Check if this pair was predicted as a link\n",
    "    found = False\n",
    "    for pred in all_predictions:\n",
    "        if (pred['doc_id'] == row['doc_id'] and \n",
    "            ((pred['entity_label'] == row['ent1'] and pred['date'] == row['ent2']) or\n",
    "             (pred['entity_label'] == row['ent2'] and pred['date'] == row['ent1']))):\n",
    "            all_test_predictions.append('LINK')\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        all_test_predictions.append('NO_LINK')\n",
    "\n",
    "# Now calculate metrics on all test pairs\n",
    "y_true_all = df['label'].tolist()\n",
    "y_pred_all = all_test_predictions\n",
    "\n",
    "print(f\"\\nAll Test Pairs Metrics:\")\n",
    "print(f\"Accuracy: {sum(1 for t, p in zip(y_true_all, y_pred_all) if t == p) / len(y_true_all):.3f}\")\n",
    "print(classification_report(y_true_all, y_pred_all, labels=['LINK', 'NO_LINK']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
